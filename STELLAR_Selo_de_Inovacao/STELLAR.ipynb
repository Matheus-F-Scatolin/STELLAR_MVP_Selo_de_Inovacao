{"cells":[{"cell_type":"markdown","metadata":{"id":"1MwnIjI4XXMB"},"source":["# **STELLAR**: A Structured, Trustworthy, and Explainable LLM-Led Architecture for Reliable Customer Support)"]},{"cell_type":"markdown","metadata":{"id":"QAy-DGlJXirS"},"source":["# 1. Requirements"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":9129,"status":"ok","timestamp":1746278273723,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"NVDEFs37XUro","outputId":"4abab03e-5262-44a9-b374-a03f0d242e66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["from google.colab import drive, userdata\n","drive.mount('/content/drive')\n","\n","# Installation of Required Libraries\n","%pip install -q -U groq langchain chromadb sentence-transformers langchain-community langchain-huggingface rank_bm25\n","# Groq API Initialization\n","from groq import Groq\n","client = Groq(api_key=userdata.get('GROQ_API_KEY'))\n","STELLAR_path = \"/content/drive/MyDrive/STELLAR_Selo_de_Inovacao\"\n","\n","# Essential Imports\n","import os\n","import json\n","import nltk\n","from math import ceil\n","from datetime import datetime\n","import time\n","import pytz\n","import requests\n","import uuid\n","import logging\n","from scipy.special import softmax\n","\n","# Library-specific Imports\n","from rank_bm25 import BM25Okapi\n","from langchain.schema.document import Document\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.vectorstores.chroma import Chroma\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    AutoConfig,\n","    logging as transformers_logging\n",")\n","\n","# Set Logging Configuration\n","logging.basicConfig(level=logging.INFO)\n","transformers_logging.set_verbosity_error()\n","\n","# Preloading Resources\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","# Model Configuration\n","MODEL = \"llama-3.3-70b-versatile\"\n","\n","\n","class Module:\n","    def __init__(self, model_name=\"llama-3.3-70b-versatile\"):\n","        self.model_name = model_name\n","\n","    def ask_model(self, question: str, model: str=\"llama-3.3-70b-versatile\") -> str:\n","        \"\"\"\n","        Queries the Groq API with a question and model.\n","\n","        Args:\n","            question (str): The input question for the model.\n","            model (str): The model to query.\n","\n","        Returns:\n","            str: The response from the model as a string or None if an error occurs.\n","        \"\"\"\n","        try:\n","            response = client.chat.completions.create(\n","                messages=[{\"role\": \"user\", \"content\": question}],\n","                model=model,\n","            )\n","            return response.choices[0].message.content\n","        except Exception as e:\n","            print(f\"Error querying Groq API: {e}\")\n","            return None\n","\n","    def load_txt_file(self, path):\n","        \"\"\"\n","        Loads a text file from the specified path.\n","\n","        Args:\n","            path (str): The path to the text file.\n","\n","        Returns:\n","            str: The contents of the text file.\n","        \"\"\"\n","        try:\n","            with open(path, 'r') as file:\n","                return file.read()\n","        except FileNotFoundError:\n","            print(f\"File not found: {path}\")\n","            return \"\"\n","\n","    def load_json_file(self, path:str) -> dict:\n","        \"\"\"\n","        Loads a JSON file from the specified path.\n","\n","        Args:\n","            path (str): The path to the JSON file.\n","\n","        Returns:\n","            dict: The contents of the JSON file as a dictionary.\n","        \"\"\"\n","        try:\n","            with open(path, 'r') as file:\n","                return json.load(file)\n","        except FileNotFoundError:\n","            print(f\"File not found: {path}\")\n","            return {}\n","\n","    def parse_str_to_json(self, string, required_fields):\n","        \"\"\"\n","        Parses a string containing a JSON-like dictionary and verifies the required fields.\n","\n","        Args:\n","            string (str): The string to parse, expected to contain a JSON-like dictionary.\n","            required_fields (list): The fields that the dictionary must contain.\n","\n","        Returns:\n","            dict: The parsed dictionary if successful.\n","\n","        Raises:\n","            ValueError: If the parsing fails or the dictionary does not contain the required fields.\n","        \"\"\"\n","        try:\n","            # Extract the JSON-like content from the string\n","            json_start = string.find(\"{\")\n","            json_end = string.find(\"}\") + 1\n","            if json_start == -1 or json_end == -1:\n","                raise ValueError(\"No valid JSON content found in the string.\")\n","\n","            json_string = string[json_start:json_end]\n","            parsed_dict = json.loads(json_string)\n","\n","            # Verify required fields\n","            if not all(field in parsed_dict for field in required_fields):\n","                raise ValueError(f\"Missing required fields: {required_fields}\")\n","\n","            return parsed_dict\n","\n","        except Exception as e:\n","            raise ValueError(f\"Error parsing string to JSON: {e}\")\n","\n","    def get_current_date_time(self, timezone:str='America/Sao_Paulo') -> dict:\n","        \"\"\"\n","        Gets the current date and time in the specified timezone.\n","\n","        Args:\n","            timezone (str): The timezone to use.\n","\n","        Returns:\n","            dict: A dictionary containing the current time, in the format:\n","              {\"year\": <int>, \"month\": <int>, \"day\": <int>, \"hour\": <int>}\n","\n","        \"\"\"\n","        timezone = pytz.timezone('America/Sao_Paulo')\n","        timezone_time = datetime.now(timezone)\n","        time = {\n","            \"year\": timezone_time.year,\n","            \"month\": timezone_time.month,\n","            \"day\": timezone_time.day,\n","            \"hour\": timezone_time.hour\n","        }\n","\n","        return time"]},{"cell_type":"markdown","metadata":{"id":"US8rxwLrYNw7"},"source":["# 2. Modules"]},{"cell_type":"markdown","metadata":{"id":"RDaSsXz0YPfO"},"source":["## 2.1 Module 1: Initial Classifier"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Run_-KFIX3pi","executionInfo":{"status":"ok","timestamp":1746278273727,"user_tz":180,"elapsed":2,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["class Module1_InitialClassifier(Module):\n","    def __init__(self, model_name:str=MODEL, prompt_path:str=f\"{STELLAR_path}/requirements/module_1/prompt_module1.txt\"):\n","        super().__init__(model_name)\n","        self.prompt_path = prompt_path\n","\n","\n","    def classify_question(self, question: str) -> dict:\n","        \"\"\"\n","        Receives a question and returns the response in JSON format. Retries up to 3 times\n","        if the response is not correctly formatted. If still invalid, returns a default answer.\n","\n","        Args:\n","            question (str): The question to send to the LLM.\n","            model (str): The model to query.\n","\n","        Returns:\n","            dict: Validated response from the LLM or a default fallback answer. format:\n","            {\"category\": <int>, \"answer\": <str>, \"explanation\": <str>, \"confidence\": <float>}\n","        \"\"\"\n","        # Get the prompt\n","        with open(self.prompt_path, \"r\") as f:\n","            prompt = f.read()\n","\n","        # Add the question to the prompt\n","        full_prompt = prompt + question\n","\n","        default_answer = {\n","            'category': 0,\n","            'answer': 'Desculpe. Houve um erro com o processamento da sua dúvida. Você está sendo encaminhado para o buscador de perguntas frequentes.',\n","            'explanation': 'Resposta padrão.',\n","            'confidence': 1\n","        }\n","\n","        for attempt in range(3):\n","            response = self.ask_model(full_prompt, self.model_name)\n","\n","            if response is None:\n","                print(f\"Attempt {attempt + 1}: No response from model.\")\n","                continue\n","\n","            parsed_response = self.parse_str_to_json(response, ['category', 'answer', 'explanation', 'confidence'])\n","            if type(parsed_response[\"category\"]) != int or parsed_response[\"category\"] not in [0, 1, 2, 3]:\n","                print(f\"Attempt {attempt + 1}: Invalid response format.\")\n","                continue\n","\n","            if parsed_response:\n","                return parsed_response\n","            else:\n","                print(f\"Attempt {attempt + 1}: Response parsing failed.\")\n","\n","        print(\"All attempts failed. Returning default answer.\")\n","        return default_answer"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":932,"status":"ok","timestamp":1746278274660,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"YHs35_q1l7gy","outputId":"3953b72b-b4c1-4814-b801-59ead7d6718b"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'category': 2, 'answer': 'Claro, podemos encaminhar sua solicitação para um atendente humano. Aguarde um momento enquanto estamos transferindo sua chamada.', 'explanation': 'O cliente explicitamente solicita falar com um atendente humano, indicando necessidade de atendimento personalizado e direto.', 'confidence': 1.0}\n"]}],"source":["# question = \"Como posso acionar meu seguro de carro?\"  # 0\n","# question = \"Onde posso ligar para cancelar meu seguro?\"  # 1\n","question = \"Preciso da ajuda de um atendente.\"  # 2\n","#question = \"Qual é o maior país do mundo?\"  # 3\n","\n","\n","module = Module1_InitialClassifier()\n","print(module.classify_question(question))"]},{"cell_type":"markdown","metadata":{"id":"JOIX25GqZTCB"},"source":["## 2.2 Module 2: RAG"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"nFL3x-1JZe_C","executionInfo":{"status":"ok","timestamp":1746278274709,"user_tz":180,"elapsed":50,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["class Module2_RAG(Module):\n","    def __init__(self, model_name=MODEL, data_path: str = f\"{STELLAR_path}/requirements/module_2/FAQs.json\",\n","                 chroma_path: str = f\"{STELLAR_path}/requirements/module_2/chroma\",\n","                 prompt_template_path: str = f\"{STELLAR_path}/requirements/module_2/prompt_template.txt\",\n","                 prompt_rerank_path: str = f\"{STELLAR_path}/requirements/module_2/prompt_rerank.txt\",\n","                 embedding_model: str = \"paraphrase-multilingual-mpnet-base-v2\",\n","                 n_retrieved_chunks: int = 10):\n","        super().__init__(model_name)\n","        self.data_path = data_path\n","        self.chroma_path = chroma_path\n","        self.prompt_template_path = prompt_template_path\n","        self.prompt_rerank_path = prompt_rerank_path\n","        self.embedding_model = embedding_model\n","        self.n_retrieved_chunks = n_retrieved_chunks\n","\n","    def add_new_faq_to_chroma(self, new_faq: dict):\n","        \"\"\"\n","        Adds a single new FAQ to the Chroma database and to the json file with all the FAQs.\n","\n","        Args:\n","            new_faq (dict): The new FAQ to be added to the database in this format:\n","                {\"category\": <str>, \"question\": <str>, \"answer\": <str> }.\n","        Returns:\n","            None\n","        \"\"\"\n","        # calculate the new chunk id (based on the existing ids)\n","        with open (self.data_path, \"r\") as f:\n","          faqs = json.load(f)\n","        max_id = max([int(faq[\"id\"]) for faq in faqs])\n","        new_id = max_id + 1\n","        new_faq[\"id\"] = new_id\n","\n","        # Format the chunk ({\"id\": <int>, \"category\": <str>, \"question\": <str>, \"answer\": <str> })\n","        formated_faq = {\n","            \"id\": new_id,\n","            \"category\": new_faq[\"category\"],\n","            \"question\": new_faq[\"question\"],\n","            \"answer\": new_faq[\"answer\"]\n","        }\n","\n","        # Write FAQ to the json file\n","        faqs.append(formated_faq)\n","        with open (self.data_path, \"w\") as f:\n","            json.dump(faqs, f, indent=2, ensure_ascii=False)\n","\n","        # Load the database\n","        path = f\"{self.chroma_path}/vector_database\"\n","        database = Chroma(\n","            persist_directory=path,\n","            embedding_function=self.get_embedding_function(self.embedding_model))\n","\n","        # Add the chunks to the database\n","        documents = [formated_faq]\n","        self.add_to_chroma(documents, model_name=self.embedding_model)\n","\n","    def create_database(self):\n","        \"\"\"\n","        Creates the Chroma database.\n","        \"\"\"\n","        # If the database already exists, return\n","        path = f\"{self.chroma_path}/vector_database\"\n","        if os.path.exists(path):\n","          return\n","        # load the items from the .json file\n","        documents = self.load_documents()\n","        # Save the chunks to the chroma directory\n","        self.add_to_chroma(documents, model_name=self.embedding_model)\n","\n","    def add_to_chroma(self, chunks: list[Document], model_name: str):\n","        \"\"\"\n","        Adds the documents to the Chroma database.\n","\n","        Args:\n","            chunks (list[Document]): The documents to add to the database in this format:\n","                {\"id\": <int>, \"category\": <str>, \"question\": <str>, \"answer\": <str>}\n","            model_name (str): The name of the embedding model to use.\n","        Returns:\n","            None\n","        \"\"\"\n","        # Load the database\n","        path = f\"{self.chroma_path}/vector_database\"\n","        database = Chroma(\n","            persist_directory=path,\n","            embedding_function=self.get_embedding_function(model_name))\n","        # Add the chunks to the database\n","        existing_items = database.get(include=[]) # IDs are included by default\n","        existing_ids = set(existing_items[\"ids\"])\n","        print(f\"Number of existing items: {len(existing_ids)}.\")\n","        # Filter out the chunks that are already in the database\n","        chunks_to_add = []\n","        for chunk in chunks:\n","            if chunk[\"id\"] not in existing_ids:\n","                chunks_to_add.append(chunk)\n","        # Add the chunks to the database\n","        if len(chunks_to_add) > 0:\n","            print(f\"Adding {len(chunks_to_add)} items to the database.\")\n","            documents_to_add = []\n","            for chunk in chunks_to_add:\n","                # Create a Document object for each chunk\n","                document = Document(\n","                    page_content=chunk[\"question\"] + \"\\n\" + chunk[\"answer\"],\n","                    metadata={\n","                        \"id\": chunk[\"id\"],\n","                        \"category\": chunk[\"category\"],\n","                        \"question\": chunk[\"question\"],\n","                        \"answer\": chunk[\"answer\"]})\n","                documents_to_add.append(document)\n","            # Add documents to the database\n","            database.add_documents(documents=documents_to_add)\n","            database.persist()\n","            print(\"✅ Database updated with new items.\")\n","        else:\n","            print(\"✅ No new items to add.\")\n","\n","    def load_documents(self) -> list[Document]:\n","        \"\"\"Loads the documents from the data directory.\n","        Documents are dictionaries with the fields:\n","        id, category, question, answer.\n","\n","        Returns:\n","            list[Document]: A list of Document objects in this format:\n","            {\"id\": <int>, \"category\": <str>, \"question\": <str>, \"answer\": <str>}\n","        \"\"\"\n","        with open(self.data_path, \"r\") as f:\n","          documents = json.load(f)\n","        return documents\n","\n","    def get_embedding_function(self, embedding_model):\n","        \"\"\"\n","        Gets the embedding function for the given embedding model.\n","        Args:\n","            embedding_model (str): The name of the embedding model.\n","        Returns:\n","            HuggingFaceEmbeddings: The embedding function.\n","        \"\"\"\n","        model = f\"sentence-transformers/{embedding_model}\"\n","        model_kwargs = {\"device\": \"cpu\"}\n","        encode_kwargs = {\"normalize_embeddings\": True}\n","        embeddings = HuggingFaceEmbeddings(\n","            model_name=model,\n","            model_kwargs=model_kwargs,\n","            encode_kwargs=encode_kwargs\n","        )\n","        return embeddings\n","\n","    def semantic_retrieval(self, question: str, n:int, model_name:str) -> list[int]:\n","        \"\"\"Receives a question and retrieves n FAQ ids related to the question\n","        Args:\n","            question (str): The question to be answered.\n","            n (int): The number of FAQs to be retrieved.\n","        Return:\n","            a list of the ids of the n FAQs related to the question\n","        \"\"\"\n","        # Prepare the database.\n","        embedding_function = self.get_embedding_function(self.embedding_model)\n","        path = f\"{self.chroma_path}/vector_database\"\n","        database = Chroma(persist_directory=path, embedding_function=embedding_function)\n","\n","        # Search the database (results:List[Tuple[Document, float]]).\n","        results = database.similarity_search_with_score(question, k=n)\n","\n","        # Return only the ids\n","        for i in range(len(results)):\n","            results[i] = results[i][0].metadata[\"id\"]\n","\n","        return results\n","\n","    def BM25_retrieval(self, question: str, n: int, list_of_chunks: list[str]) -> list[int]:\n","        \"\"\"\n","        Retrieve top-n chunks using BM25 based on the given question.\n","        Args:\n","            question (str): The query question.\n","            n (int): Number of chunks to retrieve.\n","            list_of_chunks (List[str]): The list of chunks to search within.\n","        Returns:\n","            List[int]: ids of the selected chunks\n","        \"\"\"\n","        # Tokenize the chunks and the question\n","        tokenized_chunks = [nltk.word_tokenize(chunk) for chunk in list_of_chunks]\n","        tokenized_question = nltk.word_tokenize(question)\n","\n","        # Initialize the BM25 model\n","        bm25 = BM25Okapi(tokenized_chunks)\n","\n","        # Score the chunks based on the question\n","        scores = bm25.get_scores(tokenized_question)\n","\n","        # Get the indices of the top-n chunks based on scores\n","        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:n]\n","\n","        return top_indices\n","\n","\n","    def hybrid_search(self, question: str, n: int, embedding_model: str) -> list[int]:\n","      \"\"\"Receives a question and retrieves n FAQs related to the question\n","      Return: a list of the ids of the n FAQs related to the question\n","      \"\"\"\n","      # 70% Semantic search (round up)\n","      n_semantic = ceil(0.7 * n)\n","      semantic_ids = self.semantic_retrieval(question, n_semantic, embedding_model)\n","\n","\n","      # 30% BM25 search\n","      n_bm25 = n - n_semantic\n","\n","      # Search for n chunks so that we can choose the 30% most similar that are not in semantic_ids\n","      with open (self.data_path, 'r') as file:\n","        data = json.load(file)\n","      # Use the \"question only\" approach by default\n","      chunks = [data[i][\"question\"] for i in range(len(data))]\n","      bm25_ids = self.BM25_retrieval(question, n, chunks)\n","      # Choose the 30% most similar that are not in semantic_ids\n","      bm25_ids = [i for i in bm25_ids if i not in semantic_ids]\n","      bm25_ids = bm25_ids[:n_bm25]\n","\n","      hybrid_ids = semantic_ids + bm25_ids\n","      return hybrid_ids\n","\n","    def error_handler(self, faq_ids: list[int], reranked_ids: list[int]) -> list[int]:\n","        \"\"\"\n","        Adjusts a reranked list of IDs to match the FAQ IDs list in length and content.\n","\n","        Args:\n","            faq_ids (List[int]): The original list of FAQ IDs.\n","            reranked_ids (List[int]): The reranked list of IDs.\n","\n","        Returns:\n","            List[int]: A list with the correct number of unique IDs matching FAQ IDs.\n","\n","        Raises:\n","            Exception: If reranked_ids is not a list.\n","        \"\"\"\n","\n","        # Validate input type\n","        if not isinstance(reranked_ids, list):\n","            raise Exception(\"The reranked_ids variable is not a list\")\n","\n","        # Ensure reranked_ids contains only unique integers\n","        reranked_ids = [rr_id for rr_id in reranked_ids if isinstance(rr_id, int)]\n","        reranked_ids = list(dict.fromkeys(reranked_ids))  # Remove duplicates\n","\n","        # Case 1: If sizes match, return as-is\n","        if len(reranked_ids) == len(faq_ids):\n","            return reranked_ids\n","\n","        # Case 2: If reranked_ids is smaller, fill missing IDs from faq_ids\n","        if len(reranked_ids) < len(faq_ids):\n","            missing_ids = [faq_id for faq_id in faq_ids if faq_id not in reranked_ids]\n","            return reranked_ids + missing_ids[:len(faq_ids) - len(reranked_ids)]\n","\n","        # Case 3: If reranked_ids is larger, filter out extras and recurse\n","        reranked_ids = [rr_id for rr_id in reranked_ids if rr_id in faq_ids]\n","        return self.error_handler(faq_ids, reranked_ids)\n","\n","\n","    def groq_LLM_reranking(self, query: str, faq_ids: list[int], model: str, answers:bool = True) -> list[str]:\n","        \"\"\"\n","        Reranks a list of FAQs based on their relevance to the query using Groq API.\n","\n","        Args:\n","            query (str): The question from the customer.\n","            faq_ids (List[int]): A list of the ids of the FAQs returned after a dense retrieval.\n","            model (str): The model to use for reranking.\n","\n","        Returns:\n","            List[int]: The same FAQ ids reranked by relevance.\n","        \"\"\"\n","        def load_faqs(self, faq_ids: list[int]) -> list[tuple]:\n","            \"\"\"Loads FAQs matching the given IDs from the data file.\n","            Retuns:\n","                List[Tuple[str, str, int]]\n","            \"\"\"\n","            with open(self.data_path, 'r') as file:\n","                data = json.load(file)\n","            return [(faq[\"question\"], faq[\"answer\"], faq[\"id\"]) for faq in data if faq[\"id\"] in faq_ids]\n","        def build_prompt(self, query: str, faqs_with_ids: list[tuple], answers:bool = False) -> str:\n","            \"\"\"Constructs the prompt for the model.\"\"\"\n","            with open(self.prompt_rerank_path, 'r') as file:\n","                prompt = file.read()\n","            if not answers:\n","              related_questions = \"\\n\".join(f\"{faq_id}: {content}\" for content, answer, faq_id in faqs_with_ids)\n","              return f\"{prompt}\\nDúvida do cliente:\\n{query}\\nPerguntas relacionadas:\\n{related_questions}\\n\\nSaída:\"\n","            else:\n","              related_questions = \"\\n\".join(f\"{faq_id}: {content}\\n{answer}\" for content, answer, faq_id in faqs_with_ids)\n","              return f\"{prompt}\\nDúvida do cliente:\\n{query}\\nPerguntas relacionadas:\\n{related_questions}\\n\\nSaída:\"\n","        def parse_model_response(self, response: str) -> list[int]:\n","            \"\"\"Parses the model's response into a list of IDs.\"\"\"\n","            start = response.find(\"[\")\n","            end = response.find(\"]\") + 1\n","            return json.loads(response[start:end])\n","        def call_model_with_retries(self, prompt: str, max_retries: int = 3) -> list[int]:\n","            \"\"\"Attempts to call the model with retries in case of failure.\"\"\"\n","            for attempt in range(max_retries):\n","                try:\n","                    response = self.ask_model(prompt, model)\n","                    reranked_ids = parse_model_response(self, response)\n","                    reranked_ids = self.error_handler(faq_ids, reranked_ids)\n","\n","                    if isinstance(reranked_ids, list) and len(reranked_ids) == len(faq_ids):\n","                        return reranked_ids\n","                    else:\n","                        raise ValueError(\"Invalid response format or length mismatch.\")\n","\n","                except Exception as e:\n","                    print(f\"Attempt {attempt + 1} failed: {e}\")\n","\n","            # Fall back to the original FAQ order if all retries fail\n","            print(\"Model inference failed after all retries.\")\n","            return faq_ids\n","        # Main process\n","        with open(self.prompt_rerank_path, 'r') as file:\n","            prompt = file.read()\n","        try:\n","            faqs_with_ids = load_faqs(self, faq_ids)\n","            prompt = build_prompt(self, query=query, faqs_with_ids=faqs_with_ids)\n","            return call_model_with_retries(self, prompt)\n","        except Exception as e:\n","            print(\"Unexpected error:\", e)\n","            return faq_ids\n","\n","    def query_model(self, question: str, FAQ_ids, n_ids):\n","        \"\"\"\n","        Queries the LLM with the retrieved FAQS to generate the answer to the client.\n","        Args:\n","            question (str): The question from the customer.\n","            FAQ_ids (List[int]): A list of the ids of the FAQs returned after a hybrid retrieval.\n","            n_ids (int): The number of FAQs to be retrieved.\n","\n","        Returns:\n","            str: The answer to the question.\n","        \"\"\"\n","        # Use the n_ids chunks with the highest similarity\n","        FAQ_ids = FAQ_ids[:n_ids]\n","        # create the context\n","        with open (self.data_path, 'r') as file:\n","            data = json.load(file)\n","        chunks = []\n","        for id in FAQ_ids:\n","          for faq in data:\n","            if faq[\"id\"] == id:\n","              chunks.append(faq[\"question\"]+\"\\n\"+faq[\"answer\"])\n","              break\n","\n","        context_text = \"\\n---\\n\".join([chunk for chunk in chunks])\n","        # Apply the prompt template to the query\n","        with open(self.prompt_template_path, 'r') as file:\n","            PROMPT_TEMPLATE = file.read()\n","        prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n","        prompt = prompt_template.format(context=context_text, question=question)\n","\n","        model_answer = self.ask_model(prompt, self.model_name)\n","\n","        # Format the response\n","        sources = FAQ_ids\n","        formatted_response = f\"Resposta: {model_answer}\\nFontes (ids das FAQs): {sources}\"\n","        return formatted_response\n","\n","    # Main function to handle the entire RAG process\n","    def process_question(self, question: str, n_ids: int = 5)->str:\n","        \"\"\"\n","        Retrieves FAQs from the database, reranks these FAQs\n","        and answers the question based on them.\n","\n","        Args:\n","            question (str): The question to be answered.\n","            n_ids (int): The number of FAQs to be retrieved.\n","\n","        Returns:\n","            str: The answer to the question.\n","        \"\"\"\n","        # Step 1: Retrieve IDs from the database\n","        retrieved_ids = self.hybrid_search(question, self.n_retrieved_chunks, self.embedding_model)\n","\n","        # Step 2: Re-rank the retrieved IDs\n","        reranked_ids = self.groq_LLM_reranking(\n","            question,\n","            retrieved_ids,\n","            self.model_name,\n","            answers=True\n","        )\n","\n","        # Step 3: Query the model with the top `n_ids` chunks\n","        formatted_response = self.query_model(question, reranked_ids, n_ids)\n","        return formatted_response"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":12071,"status":"ok","timestamp":1746278288227,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"8V6p_ZuRikGr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6c43169-35e8-4b3e-b0bc-425646b648f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Resposta: Os benefícios do seguro dental incluem cobertura para procedimentos como:\n","\n","- Extração\n","- Canal\n","- Cirurgias realizadas em consultório\n","- Radiologia\n","- Dentística (restaurações)\n","- Periodontia (tratamento de gengivas)\n","- Endodontia (canais)\n","- Odontopediatria (odontologia para crianças)\n","- Próteses\n","\n","Além disso, o plano Bradesco Dental oferece acesso a uma ampla rede de mais de 27 mil dentistas credenciados em todo o país, garantindo tratamentos odontológicos básicos, como limpeza e tratamento de cáries. Existem também opções específicas para crianças, como os planos Dente de Leite (0 a 7 anos) e o plano Júnior (8 a 17 anos), e um plano sem coparticipação com cobertura de várias especialidades.\n","Fontes (ids das FAQs): [42, 37, 41, 38, 39]\n"]}],"source":["module = Module2_RAG()\n","# module.create_database()\n","\n","question = \"Quais são os beneficios do seguro dental?\"\n","print(module.process_question(question))"]},{"cell_type":"markdown","metadata":{"id":"ytSguSoejHRN"},"source":["## 2.3 Module 3: Contact Info"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"g_5ahNYqjxCz","executionInfo":{"status":"ok","timestamp":1746278290770,"user_tz":180,"elapsed":11,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["class Module3_ContactInfo(Module):\n","    def __init__(self, model_name=MODEL, contact_info_path:str=f\"{STELLAR_path}/requirements/module_3/contact_info.txt\",\n","                 prompt_path:str=f\"{STELLAR_path}/requirements/module_3/initial_prompt.txt\",\n","                 followup_prompt_path:str=f\"{STELLAR_path}/requirements/module_3/followup_prompt.txt\"):\n","        super().__init__(model_name)\n","        self.contact_info = self.load_txt_file(contact_info_path)\n","        self.prompt = self.load_txt_file(prompt_path)\n","        self.followup_prompt = self.load_txt_file(followup_prompt_path)\n","        self.chat_history = \"\"\n","\n","    def ask_module(self, query:str):\n","        \"\"\"\n","        Interacts with a user to answer questions based on a predefined context and a language model.\n","\n","        This function prompts the user for a query, processes it using a predefined context template,\n","        and queries a language model for an answer. It handles follow-up questions if the model\n","        indicates ambiguity or the need for clarification.\n","        Args:\n","            query (str): The user's query.\n","        Returns:\n","            str: The final answer provided by the language model, or an appropriate message\n","                if the query cannot be answered.\n","            str: The updated chat history.\n","        \"\"\"\n","\n","        def process_follow_up(query, follow_up_question):\n","            \"\"\"\n","            Handles the process of asking a follow-up question and querying the model again.\n","            The model is not informed that it can start the answer with \"-1\". Therefore,\n","            only 1 follow-up question is asked.\n","\n","            Args:\n","                query (str): The original user query.\n","                follow_up_question (str): The follow-up question suggested by the model.\n","\n","            Returns:\n","                str: The model's final response to the query.\n","                str: The updated chat history.\n","            \"\"\"\n","            print(f\"Additional question: {follow_up_question}\")\n","            follow_up_answer = input(\"Por favor, forneça mais detalhes: \")\n","\n","            self.chat_history += f\"\\n\\033[32mUsuário\\033[0m: {follow_up_answer}\\n\"\n","\n","            full_query = query + follow_up_question + follow_up_answer\n","            follow_up_prompt = self.followup_prompt.format(context=self.contact_info, question=full_query)\n","            follow_up_response = self.ask_model(follow_up_prompt, MODEL)\n","\n","            return handle_response(follow_up_response, full_query)\n","\n","        def handle_response(response, query):\n","            \"\"\"\n","            Processes the model's response to determine the next steps.\n","\n","            Args:\n","                response (str): The model's response.\n","                query (str): The query associated with the response.\n","\n","            Returns:\n","                str: The final answer or an appropriate error message.\n","                str: The updated chat history.\n","            \"\"\"\n","            if response.startswith(\"-1\"):\n","                follow_up_question = response[3:]\n","                self.chat_history += f\"\\n\\033[34mModule 3\\033[0m: {follow_up_question}\\n\"\n","                return process_follow_up(query, follow_up_question)\n","\n","            if response.startswith(\"-2\"):\n","                return \"Infelizmente, não conseguimos achar a resposta para a sua pergunta em nossa base de contatos.\", \"\"\n","\n","\n","            return response, self.chat_history\n","\n","        # Main execution\n","        prompt = self.prompt.format(context=self.contact_info, question=query)\n","        model_answer = self.ask_model(prompt, MODEL)\n","\n","        return handle_response(model_answer, query)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1724,"status":"ok","timestamp":1746278292500,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"oT8N2gkloB2Z","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dcac1386-e4b7-46a8-959f-8ba9d497281a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Para fazer uma reclamação do seu seguro de carro, você pode ligar para o SAC (Serviço de Atendimento ao Cliente) no telefone 0800 727 9966, que está disponível 24 horas, 7 dias por semana. Além disso, você também pode entrar em contato com a Central de Relacionamento Auto, cujos números de telefone são:\n","- Capitais e Regiões Metropolitanas: 4004 2757\n","- Demais Regiões: 0800 701 2757\n","Eles atendem de segunda a sexta, das 08h às 18h (horário de Brasília).\n"]}],"source":["# query = \"Qual o número de celular para renovar meu seguro?\"\n","# query = \"Quanto é o kg do arroz?\"\n","query = \"Onde posso ligar para fazer uma reclamação do meu seguro de carro?\"\n","\n","module = Module3_ContactInfo()\n","answer, chat_history = module.ask_module(query)\n","print(answer)"]},{"cell_type":"markdown","metadata":{"id":"FMEf2c9iogFN"},"source":["## 2.4 Module 4: Human Escalation"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"nNeJMO7ootuK","executionInfo":{"status":"ok","timestamp":1746278292628,"user_tz":180,"elapsed":97,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["category_mapping = {\n","  \"Gestão de Apólices\": \"Policy Management\",\n","  \"Sinistros\":\"Claims\",\n","  \"Pagamentos\": \"Payments\",\n","  \"Perguntas Gerais\": \"General Questions\",\n","  \"Problemas Técnicos\": \"Technical Problems\",\n","  \"Escalações para Suporte Humano\": \"Human Support Escalations\",\n","  \"Perguntas Regulatórias ou de Conformidade\": \"Regulatory or Compliance Questions\"\n","}\n","\n","class Module4_HumanEscalation(Module):\n","    def __init__(self, model_name=MODEL, insurance_weights_path:str=f\"{STELLAR_path}/requirements/module_4/insurance_weights.json\",\n","                 category_weights_path:str=f\"{STELLAR_path}/requirements/module_4/category_weights.json\",\n","                 name_and_ins_type_prompt_path:str=f\"{STELLAR_path}/requirements/module_4/name_and_ins_type_prompt.txt\",\n","                 sum_and_cat_prompt_path:str=f\"{STELLAR_path}/requirements/module_4/sum_and_cat_prompt.txt\",\n","                 recommended_message_prompt_path:str=f\"{STELLAR_path}/requirements/module_4/recommended_message_prompt.txt\",\n","                 waiting_list_path:str=f\"{STELLAR_path}/outputs/module_4/waiting_list.json\",\n","                 human_agents_path:str=f\"{STELLAR_path}/human_agents/human_agents.json\"):\n","        super().__init__(model_name)\n","        self.insurance_weights = self.load_json_file(insurance_weights_path)\n","        self.category_weights = self.load_json_file(category_weights_path)\n","        self.name_and_ins_type_prompt = self.load_txt_file(name_and_ins_type_prompt_path)\n","        self.sum_and_cat_prompt = self.load_txt_file(sum_and_cat_prompt_path)\n","        self.recommended_message_prompt = self.load_txt_file(recommended_message_prompt_path)\n","        self.waiting_list_path = waiting_list_path\n","        self.human_agents_path = human_agents_path\n","\n","    def add_human_agent(self, human_agent):\n","        \"\"\"\n","        Adds a new human agent to the list of human agents.\n","        Args:\n","            human_agent (Human_agent): A dictionary containing the details of the new human agent.\n","        \"\"\"\n","        try:\n","            with open(self.human_agents_path, \"r\") as f:\n","                human_agents = json.load(f)\n","        except FileNotFoundError:\n","            human_agents = []\n","\n","        new_agent = {\n","              \"id\": (len(human_agents) + 1),\n","              \"status\": human_agent.status,\n","              \"human_attendant_name\": human_agent.human_attendant_name,\n","              \"insurance_type\": human_agent.insurance_type,\n","              \"query_category\": human_agent.query_category\n","        }\n","\n","\n","        human_agents.append(new_agent)\n","\n","        with open(self.human_agents_path, \"w\") as f:\n","            json.dump(human_agents, f, indent=2, ensure_ascii=False)\n","\n","    def free_human_agent(self, id):\n","        \"\"\"\n","        Frees an agent with the given id.\n","        Args:\n","            id (int): The id of the agent to be freed.\n","        \"\"\"\n","        try:\n","            with open(self.human_agents_path, \"r\") as f:\n","                human_agents = json.load(f)\n","        except FileNotFoundError:\n","            return\n","\n","        for agent in human_agents:\n","            if agent[\"id\"] == id:\n","                agent[\"status\"] = \"Available\"\n","                with open(self.human_agents_path, \"w\") as f:\n","                    json.dump(human_agents, f, indent=2, ensure_ascii=False)\n","                return\n","\n","    def find_available_human_agent(self)->dict:\n","        \"\"\"\n","        Finds a human agent for the customer.\n","        Returns:\n","            dict: A dictionary containing the details of the human agent in this format:\n","            {\"id\": <int>, \"status\": <str>, \"human_attendant_name\": <str>, \"insurance_type\": <int>, \"query_category\": <str>}\n","        \"\"\"\n","        try:\n","            with open(self.human_agents_path, \"r\") as f:\n","                human_agents = json.load(f)\n","        except FileNotFoundError:\n","            return None\n","\n","        # Search an agent with the same query category and insurance type\n","        for agent in human_agents:\n","            if agent[\"status\"] == \"Available\" and agent[\"insurance_type\"] == self.insurance_type and agent[\"query_category\"] == self.query_category:\n","                agent[\"status\"] = \"Busy\"\n","                with open(self.human_agents_path, \"w\") as f:\n","                    json.dump(human_agents, f, indent=2, ensure_ascii=False)\n","                return agent\n","\n","        # Search for an agent with the same insurance type\n","        for agent in human_agents:\n","            if agent[\"status\"] == \"Available\" and agent[\"insurance_type\"] == self.insurance_type:\n","                agent[\"status\"] = \"Busy\"\n","                with open(self.human_agents_path, \"w\") as f:\n","                    json.dump(human_agents, f, indent=2, ensure_ascii=False)\n","                return agent\n","        # Search for any available agents\n","        for agent in human_agents:\n","            if agent[\"status\"] == \"Available\":\n","                agent[\"status\"] = \"Busy\"\n","                with open(self.human_agents_path, \"w\") as f:\n","                    json.dump(human_agents, f, indent=2, ensure_ascii=False)\n","                return agent\n","\n","        # if no human agent is available, return None\n","        return None\n","\n","\n","\n","    def model_answer_name_and_ins_type(self, chat_history, model, max_retries=3):\n","        \"\"\"\n","          Processes a chat_history through the model and returns the response as a dictionary.\n","          Retries once if the response is not valid JSON, and falls back to parsing_error_handler.\n","          Args:\n","              chat_history (str): The input chat_history.\n","              model (str): The model to query.\n","\n","          Returns:\n","              dict: The response from the model as a dictionary in the format\n","              {\"name\": <str>, \"insurance_type\": <int>}.\n","        \"\"\"\n","        attempt = 0\n","        while attempt < max_retries:\n","            try:\n","                prompt = self.name_and_ins_type_prompt + chat_history\n","                answer = self.ask_model(prompt, model)\n","\n","                if not answer:\n","                    raise ValueError(\"Answer is None\")\n","\n","                return self.parse_str_to_json(answer, [\"name\", \"insurance_type\"])\n","\n","            except Exception as e:\n","                print(f\"Attempt {attempt + 1} failed in model_answer_name_and_ins_type: {e}\")\n","                attempt += 1\n","\n","        # if parsing failed, return the default name and ins_type\n","        output = {\n","            \"name\": \"\",\n","            \"insurance_type\": 0\n","        }\n","        return output\n","\n","    def model_answer_summary_and_category(self, chat_history, model, max_retries=3):\n","        \"\"\"\n","        Processes a chat_history through the model and returns the response as a dictionary.\n","        Retries once if the response is not valid JSON, and falls back to parsing_error_handler.\n","        Args:\n","            chat_history (str): The input chat_history.\n","            model (str): The model to query.\n","\n","        Returns:\n","            dict: The response from the model as a dictionary in the format\n","            {\"summary\": <str>, \"category\": <str>, \"subcategory\": <str>}.\n","        \"\"\"\n","        attempt = 0\n","        while attempt < max_retries:\n","            try:\n","                prompt = self.sum_and_cat_prompt + chat_history\n","                answer = self.ask_model(prompt, model)\n","\n","                if not answer:\n","                    raise ValueError(\"Answer is None\")\n","\n","                return self.parse_str_to_json(answer, [\"summary\", \"category\", \"subcategory\"])\n","\n","            except Exception as e:\n","                print(f\"Attempt {attempt + 1} failed in model_answer_summary_and_category: {e}\")\n","                attempt += 1\n","\n","        # if parsing failed, return empty summary, and default category and subcategory\n","        output = {\n","            \"summary\": \"\",\n","            \"category\": \"General Questions\",\n","            \"subcategory\": \"Other\"\n","        }\n","        return output\n","\n","\n","    def calculate_sentiment_points(self, sentiment: dict[str, float]) -> int:\n","        \"\"\"\n","        Calculates sentiment points based on the sentiment distribution.\n","        Formula:\n","            Sentiment Factor = (negative * 2.0) + (neutral * 1.0) + (positive * 0.5)\n","            Sentiment Points = min(50, max(0, Sentiment Factor * 25))\n","        Args:\n","            sentiment (Dict[str, float]): Dictionary with keys 'positive', 'neutral', 'negative' and their corresponding values.\n","        Returns:\n","            int: Sentiment points in the range [0, 50].\n","        \"\"\"\n","        sentiment_factor = (sentiment[\"negative\"] * 2.0) + (sentiment[\"neutral\"] * 1.0) + (sentiment[\"positive\"] * 0.5)\n","        sentiment_points = min(50, max(0, int(sentiment_factor * 25)))\n","        return sentiment_points\n","\n","    def calculate_category_points(self, category: str, subcategory: str, weights: dict[str, dict[str, float]]) -> int:\n","        \"\"\"\n","        Calculates category points based on predefined weights for category and subcategory.\n","        Formula:\n","            Category Points = Weight * 30\n","        Args:\n","            category (str): The category of the query.\n","            subcategory (str): The subcategory of the query.\n","            weights (Dict[str, Dict[str, float]]): Dictionary mapping categories and subcategories to their respective weights.\n","        Returns:\n","            int: Category points in the range [0, 30].\n","        \"\"\"\n","        weight = weights.get(category, {}).get(subcategory, 0.0)\n","        category_points = min(30, max(0, int(weight * 30)))\n","        return category_points\n","\n","    def calculate_insurance_points(self, insurance_type: int, weights: dict[int, float]) -> int:\n","        \"\"\"\n","        Calculates insurance points based on predefined weights for insurance types.\n","        Formula:\n","            Insurance Points = Weight * 20\n","        Args:\n","            insurance_type (int): The insurance type ID.\n","            weights (Dict[int, float]): Dictionary mapping insurance types to their respective weights.\n","        Returns:\n","            int: Insurance points in the range [0, 20].\n","        \"\"\"\n","        weight = weights.get(insurance_type, 0.0)\n","        insurance_points = min(20, max(0, int(weight * 20)))\n","        return insurance_points\n","\n","    def calculate_urgency(self, sentiment: dict[str, float], category: str, subcategory: str, insurance_type: int) -> int:\n","        \"\"\"\n","        Calculates the total urgency score based on sentiment, category/subcategory, and insurance type.\n","        Formula:\n","            Urgency Score = Sentiment Points + Category Points + Insurance Points\n","        Args:\n","            sentiment (Dict[str, float]): Sentiment analysis scores.\n","            category (str): Query category.\n","            subcategory (str): Query subcategory.\n","            insurance_type (int): Type of insurance.\n","        Returns:\n","            int: Total urgency score in the range [0, 100].\n","        \"\"\"\n","        sentiment_points = self.calculate_sentiment_points(sentiment)\n","        category_points = self.calculate_category_points(category, subcategory, self.category_weights)\n","        insurance_points = self.calculate_insurance_points(insurance_type, self.insurance_weights)\n","\n","        urgency_score = sentiment_points + category_points + insurance_points\n","        return urgency_score\n","\n","    def recommended_message(self, chat_history: str, human_attendant_name: str) -> str:\n","        \"\"\"\n","        Generates a recommended introductory message for the human attendant to send to the customer.\n","\n","        Args:\n","            chat_history (str): The history of the conversation with the customer.\n","            human_attendant_name (str): The name of the human attendant.\n","\n","        Returns:\n","            str: The recommended message for the human attendant to send to the customer.\n","        \"\"\"\n","        # Format the prompt with the input data\n","        question = self.recommended_message_prompt.format(\n","            human_attendant_name=human_attendant_name,\n","            chat_history=chat_history,\n","        )\n","\n","        # Ask the model to generate the response\n","        response = self.ask_model(question, self.model_name)\n","\n","        return response\n","\n","    def add_to_waiting_list(self, issue_data:dict):\n","        \"\"\"\n","        Adds the customer to the waiting list.\n","        Args:\n","            issue_data (dict): The data of the customer in the following format:\n","            {\"sentiment\": <dict>, \"chat_history\": <str>, \"human_attendant_name\": <str>, \"model\": <str>,\n","            \"customer_name\": <str>, \"insurance_type\": <int>, \"issue_summary\": <str>, \"query_category\": <str>,\n","            \"query_subcategory\": <str>, \"urgency_score\": <int>, \"recommended_message\": <str>}\n","        \"\"\"\n","        # Extract data from the waiting list\n","        try:\n","            with open(self.waiting_list_path, \"r\") as f:\n","                waiting_list = json.load(f)\n","        except FileNotFoundError:\n","            waiting_list = []\n","\n","        # Assume the list is ordered by urgency (first element is the most urgent one)\n","        index = 0\n","        for i in range(len(waiting_list)):\n","            # As a tiebreaker, customers who have been waiting the longest have priority.\n","            if issue_data[\"urgency_score\"] <= waiting_list[i][\"urgency_score\"]:\n","                index += 1\n","            else:\n","                break\n","\n","        waiting_list.insert(index, issue_data)\n","        with open(self.waiting_list_path, \"w\") as f:\n","            json.dump(waiting_list, f, indent=2, ensure_ascii=False)\n","\n","\n","    def run(self, chat_history:str, sentiment:dict, model=\"llama-3.3-70b-versatile\"):\n","        \"\"\"\n","        Extracts information from the chat history (customer name, insurance_type,\n","        issue summary, issue category and subcategory). After that, generates a\n","        recommended message for the human attendant to send to the customer.\n","\n","        Args:\n","            chat_history (str): The history of the conversation with the customer.\n","            sentiment (dict): Sentiment analysis scores.\n","\n","        Returns:\n","            dict: A dictionary containing the extracted information and the recommended message\n","            in the following format:\n","              {\"sentiment\": <dict>, \"chat_history\": <str>, \"human_attendant_name\": <str>,\n","              \"human_attendant_id\": <int>, \"model\": <str>, \"customer_name\": <str>, \"insurance_type\": <int>,\n","              \"issue_summary\": <str>, \"query_category\": <str>, \"query_subcategory\": <str>,\n","              \"urgency_score\": <int>, \"recommended_message\": <str>}\n","        \"\"\"\n","        output = {}\n","        output[\"sentiment\"] = sentiment\n","        output[\"chat_history\"] = chat_history\n","        output[\"model\"] = model\n","\n","        # Get Customer name and insurance type\n","        customer_name_and_ins_type = self.model_answer_name_and_ins_type(chat_history, MODEL)\n","        output[\"customer_name\"] = customer_name_and_ins_type[\"name\"]\n","        output[\"insurance_type\"] = customer_name_and_ins_type[\"insurance_type\"]\n","        self.insurance_type = customer_name_and_ins_type[\"insurance_type\"]\n","\n","        # Get issue summary and query category and query subcategory\n","        sum_cat_and_subcat = self.model_answer_summary_and_category(chat_history, MODEL)\n","        output[\"issue_summary\"] = sum_cat_and_subcat[\"summary\"]\n","        output[\"query_category\"] = sum_cat_and_subcat[\"category\"]\n","        output[\"query_subcategory\"] = sum_cat_and_subcat[\"subcategory\"]\n","        self.query_category = sum_cat_and_subcat[\"category\"]\n","\n","        # Try to find an available agent\n","        available_agent = self.find_available_human_agent()\n","        if available_agent is not None:\n","            output[\"human_attendant_name\"] = available_agent[\"human_attendant_name\"]\n","            output[\"human_attendant_id\"] = available_agent[\"id\"]\n","        else:\n","            output[\"human_attendant_name\"] = \"\"\n","            output[\"human_attendant_id\"] = \"\"\n","\n","\n","        # Calculate urgency score\n","        output[\"urgency_score\"] = self.calculate_urgency(sentiment, sum_cat_and_subcat[\"category\"], sum_cat_and_subcat[\"subcategory\"], customer_name_and_ins_type[\"insurance_type\"])\n","\n","        # Get the recommended message\n","        output[\"recommended_message\"] = self.recommended_message(chat_history, output[\"human_attendant_name\"])\n","\n","        # If there is no available human agent, add customer to the waiting list based on urgency\n","        if output[\"human_attendant_name\"] == \"\":\n","            self.add_to_waiting_list(output)\n","\n","        return output"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3845,"status":"ok","timestamp":1746278296478,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"Tb5JLIkZra7o","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c45fa36-abfc-4c18-cd79-b51b90747794"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"sentiment\": {\n","    \"positive\": 0.1,\n","    \"neutral\": 0.8,\n","    \"negative\": 0.1\n","  },\n","  \"chat_history\": \"Eu sou o Carlos, e possuo seguro de carro. Preciso falar com um atendente imediatamente.\",\n","  \"model\": \"llama-3.3-70b-versatile\",\n","  \"customer_name\": \"Carlos\",\n","  \"insurance_type\": 1,\n","  \"issue_summary\": \"Carlos é um cliente que precisa falar com um atendente humano imediatamente sobre seu seguro de carro.\",\n","  \"query_category\": \"Escalações para Suporte Humano\",\n","  \"query_subcategory\": \"Assistência Urgente\",\n","  \"human_attendant_name\": \"Maria\",\n","  \"human_attendant_id\": 2,\n","  \"urgency_score\": 56,\n","  \"recommended_message\": \"Olá, Carlos! Aqui é a Maria, da área de seguros do Bradesco Seguros. \\nVi que você precisa falar conosco com urgência sobre o seu seguro de carro e quero garantir que estamos priorizando sua solicitudade.\\nEstou aqui para ajudar e resolver qualquer problema que você esteja enfrentando o mais rápido possível.\\nVamos entender melhor o que está acontecendo com o seu seguro de carro? Pode me contar um pouco mais sobre o que você precisa ou o que está acontecendo?\"\n","}\n"]}],"source":["module = Module4_HumanEscalation()\n","\n","# Input:\n","sentiment = {\n","    \"positive\": 0.1,\n","    \"neutral\": 0.8,\n","    \"negative\": 0.1\n","}\n","chat_history = \"Eu sou o Carlos, e possuo seguro de carro. Preciso falar com um atendente imediatamente.\"\n","\n","# Output:\n","output = module.run(chat_history, sentiment)\n","print(json.dumps(output, indent=2, ensure_ascii=False))\n","module.free_human_agent(output[\"human_attendant_id\"])\n","\n","# Save to the waiting list (just to demonstrate)\n","module.add_to_waiting_list(output)"]},{"cell_type":"markdown","metadata":{"id":"5AHa-S-hG7tl"},"source":["## 2.5 Module 5: Sentiment Analysis"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"mN1ZTVJFG9oF","executionInfo":{"status":"ok","timestamp":1746278296510,"user_tz":180,"elapsed":16,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["class Module5_SentimentAnalysis(Module):\n","    def __init__(self, model_name:str=MODEL,\n","                 sentiment_model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"):\n","          super().__init__(model_name)\n","          self.sentiment_model = sentiment_model\n","\n","    def translate(self, text: str) -> str:\n","        \"\"\"\n","        Translates a given text from Portuguese to English using an LLM.\n","        Args:\n","            text (str): Input text in Portuguese.\n","        Returns:\n","            str: Translated text in English.\n","        \"\"\"\n","        prompt = (\n","            \"Translate the following Portuguese sentence to English. \"\n","            \"Respond only with the translation in a JSON format like this: {\\\"translation\\\": \\\"<your translation>\\\"}\\n\\n\"\n","            f\"Sentence: \\\"{text}\\\"\"\n","        )\n","        response = self.ask_model(prompt)\n","        try:\n","            response = response[response.index(\"{\") : response.index(\"}\") + 1]\n","            return json.loads(response)[\"translation\"]\n","        except:\n","            print(\"Invalid LLM response:\", response)\n","            return \"\"\n","\n","\n","    def sentiment_analysis(self, text: str) -> dict[str, float]:\n","        \"\"\"\n","        Analyzes the sentiment of a given text and returns sentiment scores.\n","        Args:\n","            text (str): Input text.\n","        Returns:\n","            dict[str, float]: Sentiment scores for positive, neutral, and negative classes.\n","        Example: {\"positive\": 0.03, \"neutral\": 0.92, \"negative\": 0.05}\n","        \"\"\"\n","        # Translate text to English\n","        translated_text = self.translate(text)\n","\n","        # Define tokenizer, config and model\n","        tokenizer = AutoTokenizer.from_pretrained(self.sentiment_model)\n","        config = AutoConfig.from_pretrained(self.sentiment_model)\n","        model = AutoModelForSequenceClassification.from_pretrained(self.sentiment_model)\n","\n","        # Tokenize input text\n","        encoded_input = tokenizer(text, return_tensors='pt')\n","\n","        # Perform sentiment analysis\n","        output = model(**encoded_input)\n","        scores = softmax(output[0][0].detach().numpy())\n","\n","        # Map scores to sentiment labels\n","        return {\"positive\": float(scores[2]), \"neutral\": float(scores[1]), \"negative\": float(scores[0])}"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1794,"status":"ok","timestamp":1746278319843,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"tStKwnD9IpYT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e6748d4-e307-4853-c1bb-f2211a584ab6"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'positive': 0.025908462703227997, 'neutral': 0.8828645348548889, 'negative': 0.09122706204652786}\n"]}],"source":["module = Module5_SentimentAnalysis()\n","text = \"Como posso cancelar meu seguro?\"\n","print(module.sentiment_analysis(text))"]},{"cell_type":"markdown","metadata":{"id":"X8z8ygPWMQvx"},"source":["## 2.6 Module 6: Feedback Collector"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"ioaaQgGnMZvZ","executionInfo":{"status":"ok","timestamp":1746278334347,"user_tz":180,"elapsed":26,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["class Module6_FeedbackCollector(Module):\n","    def __init__(self, chat_history:str, human_attendant_name:str, sentiment_analysis:dict,\n","                 insurance_type:int, issue_summary:str, query_category:str, query_subcategory:str,\n","                 model_name:str=MODEL, requirements_path:str=f\"{STELLAR_path}/requirements/module_6\",\n","                 comments_path:str=f\"{STELLAR_path}/outputs/module_6/comments\"):\n","        super().__init__(model_name)\n","        self.chat_history = chat_history\n","        self.human_attendant_name = human_attendant_name\n","        self.sentiment_analysis = sentiment_analysis\n","        self.insurance_type = insurance_type\n","        self.issue_summary = issue_summary\n","        self.query_category = query_category\n","        self.query_subcategory = query_subcategory\n","        self.requirements_path = requirements_path\n","        self.comments_path = comments_path\n","\n","        # Load info from the requirements path\n","        self.category_to_team = self.load_json_file(f\"{requirements_path}/category_to_team.json\")\n","        self.categorization_prompt = self.load_txt_file(f\"{requirements_path}/categorization_prompt.txt\")\n","        self.feedback_questions = self.load_json_file(f\"{requirements_path}/feedback_questions.json\")\n","        self.keys = self.load_json_file(f\"{requirements_path}/keys.json\")\n","        self.feedback_categories = self.load_json_file(f\"{requirements_path}/feedback_categories.json\")\n","\n","\n","    def categorize_comment(self, customer_comment: str) -> list[str]:\n","        \"\"\"\n","        Categorizes a customer comment into feedback categories.\n","\n","        Args:\n","            customer_comment (str): The customer comment to categorize.\n","\n","        Returns:\n","            List[str]: A list of feedback categories.\n","        \"\"\"\n","        prompt = f\"{self.categorization_prompt}\\nInput: {customer_comment}\\nOutput: \"\n","        response = self.ask_model(prompt, self.model_name)\n","        categories = []\n","        for category in self.feedback_categories:\n","            if category in response:\n","                categories.append(category)\n","\n","        if len(categories) > 0:\n","            return categories\n","        return [\"Other\"]\n","\n","\n","    def comment_routing_and_saving(self, feedback: dict):\n","        \"\"\"\n","        Routes and saves customer feedback based on their categories.\n","\n","        Args:\n","            feedback (dict): A dictionary of customer feedback.\n","        \"\"\"\n","        # Get the time\n","        time = self.get_current_date_time()\n","\n","        # Write the team-specific feedback\n","        for key, value in feedback.items():\n","            if \"categories\" in value and value[\"categories\"]:\n","                for category in value[\"categories\"]:\n","                    team = self.category_to_team.get(category, \"Customer Support Team\")\n","                    filename = f\"{self.comments_path}/{team.replace(' ', '_').lower()}_feedback.json\"\n","                    entry = {\"feedback\": value,\"feedback_type\":key, \"time\": time}\n","\n","                    try:\n","                        with open(filename, \"r\", encoding=\"utf-8\") as f:\n","                            data = json.load(f)\n","                    except FileNotFoundError:\n","                        data = []\n","\n","                    # Add the new feedback in te beggining of the list, so that newer feedback appears first\n","                    data.insert(0, entry)\n","\n","                    with open(filename, \"w\", encoding=\"utf-8\") as f:\n","                        json.dump(data, f, ensure_ascii=False, indent=2)\n","\n","        # Write the overall feedback: chat_history, human_attendant_name, sentiment_analysis, customer_name, insurance_type, issue_summary, query_category, query_subcategory\n","        path = f\"{self.comments_path}/overall_feedback.json\"\n","        try:\n","            with open(path, \"r\", encoding=\"utf-8\") as f:\n","                data = json.load(f)\n","        except FileNotFoundError:\n","            data = []\n","\n","        entry = {\"feedback\": feedback, \"time\": time}\n","        entry[\"chat_history\"] = self.chat_history\n","        if self.human_attendant_name != \"\":\n","          entry[\"human_attendant_name\"] = self.human_attendant_name\n","        if self.sentiment_analysis != {}:\n","          entry[\"sentiment_analysis\"] = self.sentiment_analysis\n","        entry[\"insurance_type\"] = self.insurance_type\n","        if self.issue_summary != \"\":\n","          entry[\"issue_summary\"] = self.issue_summary\n","        if self.query_category != \"\":\n","          entry[\"query_category\"] = self.query_category\n","        if self.query_subcategory != \"\":\n","          entry[\"query_subcategory\"] = self.query_subcategory\n","\n","        data.insert(0, entry)\n","        with open(path, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(data, f, ensure_ascii=False, indent=2)\n","\n","        return None\n","\n","\n","    def get_customer_feedback(self) -> dict:\n","        \"\"\"\n","        Collects customer feedback by asking multiple-choice questions (1-5 scale) and optional follow-up questions.\n","\n","        Returns:\n","            dict: Feedback containing ratings, optional follow-up responses for each question in this format:\n","            {\n","                key: {\n","                    \"rating\": <int>,\n","                    \"follow_up_response\": <str>,\n","                    \"categories\": [<str>]\n","                },\n","        \"\"\"\n","        feedback = {}\n","\n","        for key, question in zip(self.keys, self.feedback_questions):\n","            while True:\n","                try:\n","                    rating = int(input(f\"{question} (1-5): \"))\n","                    if rating < 1 or rating > 5:\n","                        print(\"Por favor, digite um número entre 1 e 5.\")\n","                        continue\n","                    break\n","                except ValueError:\n","                    print(\"Entrada Inválida. Por favor, digite um número entre 1 e 5.\")\n","\n","            follow_up_response = None\n","            if rating <= 3:\n","                follow_up_response = input(\n","                    \"Sentimos muito por isso. Você poderia nos contar o que poderia ter sido melhor? \")\n","\n","            feedback[key] = {\n","                \"rating\": rating,\n","                \"follow_up_response\": follow_up_response\n","            }\n","\n","        # the general rating is the average of the other ratings\n","        general_rating = round(sum([feedback[key][\"rating\"] for key in self.keys]) / len(self.keys))\n","        # Ask for optional general comment\n","        comment = input(\"Você gostaria de fazer algum comentário adicional sobre o sua experiência? \")\n","        feedback[\"general\"] = {\"rating\": general_rating, \"follow_up_response\": comment}\n","\n","        # Categorize each written comment\n","        for key, value in feedback.items():\n","          if value[\"follow_up_response\"] is not None and len(value[\"follow_up_response\"]) > 5:\n","            feedback[key][\"categories\"] = self.categorize_comment(value[\"follow_up_response\"])\n","          else:\n","            feedback[key][\"categories\"] = []\n","\n","        # route comments\n","        self.comment_routing_and_saving(feedback)\n","\n","        return feedback"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":1239,"status":"ok","timestamp":1746278335585,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"2h6lnPAKOnsf"},"outputs":[],"source":["chat_history = \"Olá, sou o Matheus. Estou muito frustrado com a demora para acionamento do meu seguro de saúde!\"\n","human_attendant_name = \"Gabriel\"\n","sentiment = {\n","    \"positive\": 0.022862698882818222,\n","    \"neutral\": 0.031627094745636,\n","    \"negative\": 0.945510176569223404\n","}\n","insurance_type = 2\n","issue_summary = \"Matheus precisa de ajuda urgente com o acionamento do seguro de saúde.\"\n","query_category = \"Sinistros\"\n","query_subcategory = \"Abertura de Sinistro\"\n","\n","module = Module6_FeedbackCollector(chat_history, human_attendant_name, sentiment, insurance_type, issue_summary, query_category, query_subcategory)\n","#feedback = module.get_customer_feedback()"]},{"cell_type":"markdown","metadata":{"id":"Suddcf6bRY9z"},"source":["## 2.7 Module 7: Knowledge Base Builder"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"2elNw2-9RjUH","executionInfo":{"status":"ok","timestamp":1746278335589,"user_tz":180,"elapsed":3,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["class Module7_KnowledgeBaseBuilder(Module):\n","    def __init__(self, chat_history:str, insurance_type:int, model_name:str=MODEL, prompt_path:str=f\"{STELLAR_path}/requirements/module_7/prompt.txt\",\n","                 review_query_path:str=f\"{STELLAR_path}/outputs/module_7/review_query.json\",\n","                 int_to_ins_type_path:str=f\"{STELLAR_path}/requirements/module_7/int_to_ins_type.json\"):\n","        super().__init__(model_name)\n","        self.chat_history = chat_history\n","        self.insurance_type = insurance_type\n","        self.model_name = model_name\n","        self.review_query_path = review_query_path\n","\n","        self.prompt = self.load_txt_file(prompt_path)\n","        self.int_to_ins_type = self.load_json_file(int_to_ins_type_path)\n","\n","\n","    def add_to_review_queue(self, draft_faq_entry: dict) -> None:\n","        \"\"\"\n","        Adds generated draft FAQ entries to a queue for human review.\n","\n","        Args:\n","          draft_faq_entry (list): A list of dictionaries representing draft FAQ entries.\n","        \"\"\"\n","        try:\n","          with open(self.review_query_path, \"r\") as f:\n","              review_query = json.load(f)\n","        except:\n","          print(\"Error loading existing review query. Creating new one.\")\n","          review_query = []\n","\n","        time = self.get_current_date_time()\n","\n","        new_faq_entry = {\n","            \"draft_faq\": draft_faq_entry,\n","            \"chat_history\": self.chat_history,\n","            \"time\": time,\n","            \"status\": \"pending\"\n","        }\n","        review_query.append(new_faq_entry)\n","        try:\n","          with open(self.review_query_path, \"w\") as f:\n","              json.dump(review_query, f, indent=4, ensure_ascii=False)\n","        except:\n","            print(\"Error writing to review query file. Changes not saved.\")\n","\n","\n","    def parse_response(self, response:str)->dict:\n","        \"\"\"\n","        Parses the response from the model and extracts a dict of fields \"question\"\n","        and \"draft_answer\". If the answer is not in json format or the fields are\n","        not present, the function raises an exception.\n","\n","        Args:\n","            response (str): The response from the model as a string.\n","\n","        Returns:\n","            dict: A dictionary containing the parsed fields.\n","\n","        Raises:\n","            Exception: If the response is not in JSON format or the fields are missing.\n","        \"\"\"\n","        try:\n","            json_data = json.loads(response)\n","            if \"question\" not in json_data or \"draft_answer\" not in json_data:\n","                raise Exception(\"Missing 'question' or 'draft_answer' field in LLM response.\")\n","            if not isinstance(json_data[\"question\"], str) or not isinstance(json_data[\"draft_answer\"], str):\n","                raise Exception(\"Invalid data type for 'question' or 'draft_answer' field.\")\n","            if not json_data[\"question\"].strip() or not json_data[\"draft_answer\"].strip():\n","                raise Exception(\"Empty 'question' or 'draft_answer' field.\")\n","            if len(json_data) != 2:\n","                raise Exception(\"LLM response has more than 2 fields.\")\n","            return json_data\n","        except json.JSONDecodeError:\n","            # if the response has more than 100 characters, truncate it\n","            if len(response) > 100:\n","                response = response[:100] + \"...\"\n","            print(\"Response: \" + response)\n","            raise Exception(\"LLM response is not in JSON format.\")\n","\n","\n","    def generate_draft_faq(self)->dict:\n","        \"\"\"\n","        Creates a new draft FAQ entry in the database. Allow a maximum of 2 retries.\n","\n","        Returns:\n","            dict: A dictionary containing the fields \"question\" and \"draft_answer\".\n","            If the draft FAQ cannot be generated after 3 attempts, returns None.\n","        \"\"\"\n","        retries = 0\n","        while retries < 3:\n","            try:\n","                prompt = self.prompt + f\"\\n{self.chat_history}\"\n","                response = self.ask_model(prompt, self.model_name)\n","                draft_faq = self.parse_response(response)\n","                draft_faq[\"category\"] = self.int_to_ins_type.get(str(self.insurance_type), \"default\")\n","\n","                self.add_to_review_queue(draft_faq)\n","                return draft_faq\n","            except Exception as e:\n","                retries += 1\n","                if retries == 3:\n","                    raise Exception(f\"Failed to generate draft FAQ after {retries} attempts: {e}\")\n","                print(f\"Error generating draft FAQ: {e}. Retrying...\")\n","\n","        return None\n","\n","\n","    def review_pending_faqs(self) -> None:\n","        \"\"\"\n","        Reviews pending FAQ entries and allows the human agent to approve, disapprove, rewrite, or leave them as pending.\n","        Approved or disapproved FAQs are saved in the respective files and removed from the review queue.\n","        \"\"\"\n","        try:\n","            with open(self.review_query_path, \"r\") as f:\n","                review_query = json.load(f)\n","        except FileNotFoundError:\n","            print(\"No pending FAQs found.\")\n","            return\n","\n","        updated_review_query = []\n","        for entry in review_query:\n","            if entry[\"status\"] != \"pending\":\n","                updated_review_query.append(entry)\n","                continue\n","\n","            print(\"FAQ review:\")\n","            print(f\"Question: {entry['draft_faq']['question']}\")\n","            print(f\"Draft Answer: {entry['draft_faq']['draft_answer']}\")\n","            print(f\"Category: {entry['draft_faq']['category']}\")\n","            print(\"\\nOptions\")\n","            print(\"a) Rewrite the answer.\")\n","            print(\"b) Approve FAQ without changes.\")\n","            print(\"c) Disapprove the FAQ.\")\n","            print(\"d) Skip to the next FAQ (and leave it pending).\")\n","            decision = input(\"Enter your choice (a/b/c/d): \").strip().lower()\n","\n","            if decision == \"a\":\n","                new_answer = input(\"Enter your answer to the question: \").strip()\n","                entry[\"draft_faq\"][\"draft_answer\"] = new_answer\n","                entry[\"status\"] = \"approved\"\n","                approved_path = self.review_query_path.replace(\"review_query\", \"approved_faqs\")\n","                self.save_faq_and_upload_chroma(entry, approved_path)\n","\n","\n","            elif decision == \"b\":\n","                entry[\"status\"] = \"approved\"\n","                approved_path = self.review_query_path.replace(\"review_query\", \"approved_faqs\")\n","                self.save_faq_and_upload_chroma(entry, approved_path)\n","\n","            elif decision == \"c\":\n","                entry[\"status\"] = \"rejected\"\n","                rejected_path = self.review_query_path.replace(\"review_query\", \"rejected_faqs\")\n","                self.save_faq_and_upload_chroma(entry, rejected_path)\n","\n","            elif decision == \"d\":\n","                print(\"Keeping the status as pending.\")\n","                updated_review_query.append(entry)\n","\n","            else:\n","                print(\"Invalid input. Keeping the status as pending.\")\n","                updated_review_query.append(entry)\n","\n","        with open(self.review_query_path, \"w\") as f:\n","            json.dump(updated_review_query, f, indent=2, ensure_ascii=False)\n","\n","    def save_faq_and_upload_chroma(self, entry: dict, path: str) -> None:\n","        \"\"\"\n","        Saves the approved or disapproved FAQ to the respective JSON file.\n","        If the FAQ was approved, adds it to the FAQ vector database.\n","\n","        Args:\n","            entry (dict): The FAQ entry to save in this format:\n","                {\"draft_faq\": {\"question\": <str>, \"draft_answer\": <str>, \"category\": <str>}}\n","            path (str): The path to the JSON file where the FAQ should be saved.\n","        \"\"\"\n","        # Save to the Chroma vector database\n","        if entry[\"status\"] == \"approved\":\n","            new_faq = entry[\"draft_faq\"]\n","\n","            formated_faq = {\n","                \"category\": new_faq[\"category\"],\n","                \"question\": new_faq[\"question\"],\n","                \"answer\": new_faq[\"draft_answer\"]\n","            }\n","            self.update_faq_vector_database(formated_faq)\n","\n","        # Save to the json file\n","        try:\n","            with open(path, \"r\") as f:\n","                faqs = json.load(f)\n","        except FileNotFoundError:\n","            faqs = []\n","\n","        faqs.append(entry)\n","        with open(path, \"w\") as f:\n","            json.dump(faqs, f, indent=2, ensure_ascii=False)\n","\n","\n","    def update_faq_vector_database(self, approved_faq: dict) -> None:\n","        \"\"\"\n","        Updates the FAQ vector database with a single new approved FAQ by calling Module2's add_new_faq_to_chroma function.\n","\n","        Args:\n","            approved_faq (dict): The approved FAQ entry to add to the vector database in this format:\n","                {\"category\": <str>, \"question\": <str>, \"draft_answer\": <str> }.\n","        \"\"\"\n","        print(\"Updating FAQ vector database...\")\n","        module_2 = Module2_RAG()\n","        module_2.add_new_faq_to_chroma(approved_faq)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1638,"status":"ok","timestamp":1746278337228,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"NfuWIy0vx_JT","outputId":"e3decd59-650c-46a3-d289-e3079c958fed"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","    \"question\": \"Como encontrar dentistas credenciados perto de mim para acionar meu seguro dental?\",\n","    \"draft_answer\": \"Você pode encontrar dentistas credenciados em nosso site ou através da nossa Central de Relacionamento, que podem fornecer informações sobre a rede de profissionais credenciados da Bradesco Seguros.\",\n","    \"category\": \"dental_insurance\"\n","}\n"]}],"source":["chat_history = \"Preciso acionar meu seguro dental, mas não sei como faço para encontrar dentistas credenciados perto de mim.\"\n","ins_type = 4\n","\n","module = Module7_KnowledgeBaseBuilder(chat_history, ins_type)\n","draft_faq = module.generate_draft_faq()\n","print(json.dumps(draft_faq, indent=4, ensure_ascii=False))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"GKWQxXXtyFut","executionInfo":{"status":"ok","timestamp":1746278337233,"user_tz":180,"elapsed":4,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["module_2 = Module2_RAG()\n","new_draft_faq = {\n","    \"question\": \"Como acionar meu seguor viagem em caso de bagagem desaparecida?\",\n","    \"answer\": \"Você pode acessar nosso app, ir em 'meus seguros'. 'seguro viagem' e 'perdi minha bagagem'.\",\n","    \"category\": \"travel_insurance\"\n","}\n","#module_2.add_new_faq_to_chroma(new_draft_faq)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Q16F0JsPy9Rq","executionInfo":{"status":"ok","timestamp":1746278337250,"user_tz":180,"elapsed":14,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["module_7 = Module7_KnowledgeBaseBuilder(chat_history =\"\", insurance_type=None)\n","#module_7.review_pending_faqs()"]},{"cell_type":"markdown","metadata":{"id":"DOsKCQFazH5q"},"source":["## 2.8. Module 8: Resolution Verifier"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"YL28ziZGziPc","executionInfo":{"status":"ok","timestamp":1746278337317,"user_tz":180,"elapsed":45,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["class Module8_ResolutionVerifier(Module):\n","    def __init__(self, chat_history:str, model_name:str=MODEL, prompt_path:str=f\"{STELLAR_path}/requirements/module_8/prompt_resolution_verifier_question.txt\"):\n","        super().__init__(model_name)\n","        self.chat_history = chat_history\n","        self.prompt_path = prompt_path\n","        self.prompt = self.load_txt_file(self.prompt_path)\n","\n","    def generate_verification_question(self) -> str:\n","        \"\"\"\n","        Generates a tailored verification question for the customer based on the parsed chat history.\n","\n","        Returns:\n","            str: A personalized question to verify if the issue was resolved.\n","        \"\"\"\n","        prompt = self.prompt.format(chat_history=self.chat_history)\n","        return self.ask_model(prompt, self.model_name)\n","\n","    def verify_issue_resolution(self, verification_question:str) -> tuple[bool, str]:\n","        \"\"\"\n","        Verifies with the customer if the issue was resolved based on the provided verification question.\n","\n","        Args:\n","            verification_question (str): The question to verify if the issue was resolved.\n","\n","        Returns:\n","            bool: True if the issue was resolved, False otherwise.\n","            str: The chat history from the module with the verification question and answer.\n","        \"\"\"\n","        chat_history = \"\\n\\033[34mModule 8\\033[0m: \" + verification_question\n","        answer = \"\"\n","        while answer != \"S\" and answer != \"N\":\n","            answer = input(verification_question + \" (S/N): \\n\")\n","            if answer != \"\":\n","                answer = answer.upper().strip()[0]\n","            if answer != \"S\" and answer != \"N\":\n","                print(\"Resposta Inválida. Favor responder com 'S' ou 'N' somente.\")\n","\n","        chat_history += \"\\n\\033[32mUser\\033[0m: \" + answer\n","        return answer == \"S\", chat_history\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"W725t5Fj0QCh","executionInfo":{"status":"ok","timestamp":1746278337926,"user_tz":180,"elapsed":608,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1fed51af-8359-47b3-e8b6-0520cb1a1a2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["José, você conseguiu encontrar a opção de acionamento do seu seguro de carro no App Bradesco Seguros?\n"]}],"source":["chat_history = \"\"\"\"Olá, eu sou o José. Estou com uma dúvida relacionada ao acionamento do meu seguro de Carro. Onde encontro a opção de acionamento?\n","Oi, José! Eu sou o assistente virtual do Bradesco Seguros. Com base em uma busca à nossa base de dados, no seu caso, Você deve entrar no App Bradesco Seguros e entrar na página \"meus seguros\", onde encontrará as informações sopbre o acionamento do seu seguro de carro.\"\"\"\n","module = Module8_ResolutionVerifier(chat_history=chat_history)\n","question = (module.generate_verification_question())\n","print(question)"]},{"cell_type":"markdown","metadata":{"id":"aJCjG7U70iAZ"},"source":["## 2.9 Module 9: Compliance Verifier"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"vZSAPOk40hZ8","executionInfo":{"status":"ok","timestamp":1746278337928,"user_tz":180,"elapsed":4,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["class Module9_ComplianceVerifier(Module):\n","    def __init__(self, user_question:str, llm_response:str, model_name:str=\"gemma2-9b-it\",\n","                 prompt_path:str=f\"{STELLAR_path}/requirements/module_9/prompt_compliance_verifier_1.txt\",\n","                 log_path:str=f\"{STELLAR_path}/outputs/module_9/logs/violations.json\"):\n","        super().__init__(model_name)\n","        self.user_question = user_question\n","        self.llm_response = llm_response\n","        self.prompt_path = prompt_path\n","        self.log_path = log_path\n","        self.prompt = self.load_prompt()\n","\n","    def load_prompt(self):\n","        with open(self.prompt_path, \"r\") as f:\n","            prompt = f.read()\n","        return prompt\n","\n","    def format_input(self) -> str:\n","        \"\"\"\n","        Formats the input for the LLM by combining the chat history, LLM response, and the loaded prompt.\n","        Returns the formatted input string ready to be sent to the model.\n","        \"\"\"\n","        dict_input = {\"query\":self.user_question, \"response\":self.llm_response}\n","        prompt = self.prompt + json.dumps(dict_input, indent=2, ensure_ascii=False)\n","        return prompt\n","\n","    def parse_response(self, llm_output: str) -> dict:\n","        \"\"\"\n","        Parses the output from the model and extracts a dict of fields \"compliance\"\n","        and \"violation\". If the answer is not in json format or the fields are\n","        not present, the function raises an exception.\n","\n","        Args:\n","            llm_output (str): The output from the model as a string.\n","\n","        Returns:\n","            dict: A dictionary containing the parsed fields.\n","\n","        Raises:\n","            Exception: If the response is not in JSON format or the fields are missing.\n","        \"\"\"\n","        try:\n","            if llm_output is None:\n","                raise Exception(\"LLM output is None.\")\n","            dict_response = llm_output[llm_output.index(\"{\"):llm_output.index(\"}\")+1]\n","            json_data = json.loads(dict_response)\n","            if \"compliance\" not in json_data or \"violation\" not in json_data:\n","                raise Exception(\"Missing 'compliance' or 'violation' field in LLM response.\")\n","            if not isinstance(json_data[\"compliance\"], bool) or not isinstance(json_data[\"violation\"], str):\n","                raise Exception(\"Invalid data type for 'compliance' or 'violation' field.\")\n","            if not json_data[\"violation\"].strip():\n","                raise Exception(\"Empty 'violation' field.\")\n","            if len(json_data) != 2:\n","                raise Exception(\"LLM response has more than 2 fields.\")\n","            return json_data\n","        except json.JSONDecodeError:\n","            print(\"Response: \" + llm_output)\n","            raise Exception(\"LLM response is not in JSON format.\")\n","\n","\n","    def run_verification(self) -> dict:\n","        \"\"\"\n","        Executes the compliance verification process.\n","        Combines input formatting, sending the request to the model, and analyzing the response.\n","        Returns:\n","            dict: The final compliance analysis, including compliance status and violations in this format:\n","                {\"compliance\": <bool>, \"violation\": <str>}\n","\n","        \"\"\"\n","        input = self.format_input()\n","        llm_output = self.ask_model(input, self.model_name)\n","        for i in range(3):\n","            if llm_output is None:\n","                print(f\"LLM output is None, retrying... ({i+1}/3)\")\n","                continue\n","            try:\n","                results = self.parse_response(llm_output)\n","                self.log_violations(results)\n","                return results\n","            except Exception as e:\n","                print(f\"Error parsing response: {e}\")\n","\n","        return {\"compliance\": False, \"violation\": f\"ERROR: unable to parse the LLM response: {llm_output}\"}\n","\n","    def log_violations(self, results: dict) -> None:\n","        \"\"\"\n","        Logs any detected violations to a specified log file.\n","        Args:\n","            results (dict): The compliance verification results, including any violations.\n","        \"\"\"\n","        # If there is not a violation, return\n","        if results[\"compliance\"]:\n","            return\n","\n","        # If there is a violation, log it\n","        # Set the time\n","        time = self.get_current_date_time()\n","\n","        # Open the log file\n","        try:\n","          with open(self.log_path, \"r\") as f:\n","              log_data = json.load(f)\n","        except FileNotFoundError:\n","            log_data = []\n","\n","        log = {\n","            \"id\": len(log_data) + 1,\n","            \"time\": time,\n","            \"query\": self.user_question,\n","            \"response\": self.llm_response,\n","            \"violation\": results[\"violation\"]\n","        }\n","\n","        log_data.append(log)\n","\n","        # Save the log data\n","        with open(self.log_path, \"w\") as f:\n","            json.dump(log_data, f, indent=2, ensure_ascii=False)\n"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":1117,"status":"ok","timestamp":1746278339045,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"cZXj77VH3NIn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"31772763-ac7d-4705-e68a-3947ea3e5f1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'compliance': False, 'violation': 'Resposta incompleta ou vaga'}\n","{'compliance': True, 'violation': 'Sem violação'}\n","{'compliance': False, 'violation': 'Tom inadequado'}\n"]}],"source":["module = Module9_ComplianceVerifier(user_question=\"Como consigo fazer o cancelamento do meu seguro dental?\", llm_response=\"Infelizmente não tenho a resposta para sua pergunta.\")\n","print(module.run_verification())\n","\n","module = Module9_ComplianceVerifier(user_question=\"Como consigo fazer a renovação do meu seguro de carro?\", llm_response=\"Basta enviar um email para renovacoes_auto@bradesco.com.\")\n","print(module.run_verification())\n","\n","module = Module9_ComplianceVerifier(user_question=\"Como consigo fazer a ativação do meu seguro de viagem?\", llm_response=\"Se vira aí.\")\n","print(module.run_verification())"]},{"cell_type":"markdown","metadata":{"id":"cb5g8uaP3s6l"},"source":["# 3. System"]},{"cell_type":"markdown","metadata":{"id":"jW_jE2AX-TzR"},"source":["## 3.1 CUSTOMER (query workflow)"]},{"cell_type":"markdown","metadata":{"id":"LPcmSxbCLyUM"},"source":["This is what the customer sees and how he interacts with the system"]},{"cell_type":"markdown","metadata":{"id":"_7ogq68YH4R-"},"source":["### 3.3.1 Workflow Implementation"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"3tLFHw_A4bIt","executionInfo":{"status":"ok","timestamp":1746278339414,"user_tz":180,"elapsed":31,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["def send_message_to_the_user(message):\n","    print(message)\n","\n","def get_user_input(prompt):\n","    return input(prompt)\n","\n","class WorkflowManager:\n","    def __init__(self):\n","        self.chat_history:str = \"\"\n","        self.logs_path = f\"{STELLAR_path}/logs/workflow_logs.json\"\n","        self.sequence_of_modules = []\n","\n","    def save_logs(self, logs):\n","        try:\n","            with open(self.logs_path, \"r\") as f:\n","                logs_data = json.load(f)\n","        except FileNotFoundError:\n","            logs_data = []\n","\n","        logs_data.append(logs)\n","        with open(self.logs_path, \"w\") as f:\n","            json.dump(logs_data, f, indent=2, ensure_ascii=False)\n","\n","\n","    def _log_execution(self, module_number, start_time, response):\n","        execution_time = time.time() - start_time\n","        log_entry = {\n","            \"module\": module_number,\n","            \"execution_time\": execution_time,\n","            \"timestamp\": datetime.now().isoformat(),\n","            \"response\": response\n","        }\n","        return log_entry\n","\n","    def process_query(self, query):\n","        \"\"\"\n","        Args:\n","            query (str): customer's initial query\n","        Returns:\n","            dict: a dict with the final state of the program and the execution logs in this format:\n","                {\"chat_history\": <str>,\n","                \"sequence_of_modules\": <list>,\n","                \"final_state\": <dict>,\n","                \"execution_logs\": <list>}\n","        \"\"\"\n","        logs = []\n","        current_state = {}\n","        self.chat_history += f\"\\n\\033[32mUser\\033[0m: {query}\"\n","\n","        # Start with Module 1\n","        self.sequence_of_modules.append(1)\n","        module1 = Module1_InitialClassifier()\n","        start_time = time.time()\n","        initial_classification:int = module1.classify_question(query)['category']\n","        logs.append(self._log_execution(1, start_time, initial_classification))\n","        current_state['classification'] = initial_classification\n","\n","        # Branch based on Module 1's classification\n","        if initial_classification == 0:  # FAQ path\n","            send_message_to_the_user(\"Obrigado pela pergunta! Estamos buscando uma resposta para ela no banco de perguntas frequentes.\")\n","            self.chat_history += f\"\\n\\033[36mSistema\\033[0m: Estamos buscando uma resposta para ela no banco de perguntas frequentes.\"\n","            current_state = self._handle_faq_path(query, current_state, logs)\n","        elif initial_classification == 1:  # Contact info path\n","            send_message_to_the_user(\"Obrigado pela pergunta! Estamos buscando uma resposta para ela no banco de informações de contato.\")\n","            self.chat_history += f\"\\n\\033[36mSistema\\033[0m: Obrigado pela pergunta! Estamos buscando uma resposta para ela no banco de informações de contato.\"\n","            current_state = self._handle_contact_path(query, current_state, logs)\n","        elif initial_classification == 2:  # Human escalation path\n","            send_message_to_the_user(\"Vamos te encaminhar a um atendente humano. Aguarde um instante.\")\n","            self.chat_history += \"\\n\\033[36mSistema\\033[0m: Vamos te encaminhar a um atendente humano. Aguarde um instante.\"\n","            current_state = self._handle_human_escalation_path(query, current_state, logs)\n","        else: # Question was tagged as not relevant\n","            print(\"Pergunta Não pertinente.\")\n","            return\n","\n","\n","\n","        full_log = {\n","            \"chat_history\": self.chat_history,\n","            \"sequence_of_modules\": self.sequence_of_modules,\n","            \"final_state\": current_state,\n","            \"execution_logs\": logs\n","        }\n","        self.save_logs(full_log)\n","        return full_log\n","\n","    def _handle_faq_path(self, query, state, logs):\n","\n","        # Module 2 - RAG\n","        self.sequence_of_modules.append(2)\n","        module2 = Module2_RAG()\n","        start_time = time.time()\n","        rag_response:str = module2.process_question(query)\n","        logs.append(self._log_execution(2, start_time, rag_response))\n","        state['rag_response'] = rag_response\n","\n","\n","        # Module 9 - Compliance\n","        self.sequence_of_modules.append(9)\n","        module9 = Module9_ComplianceVerifier(query, rag_response)\n","        start_time = time.time()\n","        compliance_check:dict[bool,str] = module9.run_verification()\n","        logs.append(self._log_execution(9, start_time, compliance_check))\n","\n","        if not compliance_check['compliance']:\n","            rag_response = self._handle_compliance_failure_faq_path(query, state, logs)\n","            if rag_response == \"\":\n","                send_message_to_the_user(\"Infelizmente, houve um erro na busca pela sua resposta no banco de perguntas frequentes. Vamos te encaminhar a um atendente humano.\")\n","                self.chat_history += \"\\n\\033[36mSistema\\033[0m: Infelizmente, houve um erro na busca pela sua resposta. Vamos te encaminhar a um atendente humano.\"\n","                return self._handle_human_escalation_path(query, state, logs)\n","\n","        send_message_to_the_user(rag_response)\n","        self.chat_history += f\"\\n\\033[34mModulo 2\\033[0m: {rag_response}\"\n","\n","        # Module 8 - Resolution\n","        self.sequence_of_modules.append(8)\n","        module8 = Module8_ResolutionVerifier(self.chat_history)\n","        start_time = time.time()\n","        verification_question = module8.generate_verification_question()\n","        logs.append(self._log_execution(8, start_time, verification_question))\n","\n","        resolution_check, chat_history_module8 = module8.verify_issue_resolution(verification_question)\n","        self.chat_history += chat_history_module8\n","        state['resolution_check_faq_path'] = resolution_check\n","\n","\n","        if not resolution_check:\n","            send_message_to_the_user(\"É uma pena sua questão não ter sido resolvida. Vamos te encaminhar a um atendente humano.\")\n","            self.chat_history += \"\\n\\033[36mSistema\\033[0m: É uma pena sua questão não ter sido resolvida. Vamos te encaminhar a um atendente humano.\"\n","            return self._handle_human_escalation_path(query, state, logs)\n","\n","        # If the issue is resolved, collect feedback\n","        send_message_to_the_user(\"Que bom que sua questão tenha sido resolvida! Agora, poderia nos fornecer uma avaliação rápida para melhorar o atendimento?\")\n","        self.chat_history += \"\\n\\033[36mSistema\\033[0m: Que bom que sua questão tenha sido resolvida! Agora, poderia nos fornecer uma avaliação rápida para melhorar o atendimento?\"\n","\n","        return self._collect_feedback(query, state, logs)\n","\n","    def _handle_contact_path(self, query, state, logs):\n","        # Module 3 - Contact Info\n","        self.sequence_of_modules.append(3)\n","        module3 = Module3_ContactInfo()\n","        start_time = time.time()\n","        contact_response, chat_history_module3 = module3.ask_module(query)\n","        logs.append(self._log_execution(3, start_time, contact_response))\n","        state['contact_response'] = contact_response\n","        self.chat_history += chat_history_module3\n","\n","        # Module 9 - Compliance\n","        self.sequence_of_modules.append(9)\n","        module9 = Module9_ComplianceVerifier(query, contact_response)\n","        start_time = time.time()\n","        compliance_check:dict[bool,str] = module9.run_verification()\n","        logs.append(self._log_execution(9, start_time, compliance_check))\n","\n","        if not compliance_check['compliance']:\n","            contact_response = self._handle_compliance_failure_contact_info_path(query, state, logs)\n","            if contact_response == \"\":\n","                send_message_to_the_user(\"Infelizmente, houve um erro na busca pela sua resposta. Vamos tentar procurá-la no banco de perguntas frequentes.\")\n","                self.chat_history += \"\\n\\033[36mSistema\\033[0m: Infelizmente, houve um erro na busca pela sua resposta. Vamos tentar procurá-la no banco de perguntas frequentes.\"\n","                return self._handle_faq_path(query, state, logs)\n","\n","        send_message_to_the_user(contact_response)\n","        self.chat_history += f\"\\n\\033[34mModulee 3\\033[0m: {contact_response}\"\n","\n","        # Module 8 - Resolution\n","        self.sequence_of_modules.append(8)\n","        module8 = Module8_ResolutionVerifier(self.chat_history)\n","        start_time = time.time()\n","        verification_question = module8.generate_verification_question()\n","        logs.append(self._log_execution(8, start_time, verification_question))\n","        resolution_check, chat_history_module8 = module8.verify_issue_resolution(verification_question)\n","        self.chat_history += chat_history_module8\n","        state['resolution_check_contact_path'] = resolution_check\n","\n","\n","        if not resolution_check:\n","            # Try FAQ path as fallback\n","            send_message_to_the_user(\"É uma pena sua questão não ter sido resolvida. Vamos te encaminhar ao sistema de busca de perguntas frequentes.\")\n","            self.chat_history += \"\\n\\033[36mSistema\\033[0m: É uma pena sua questão não ter sido resolvida. Vamos te encaminhar ao sistema de busca de perguntas frequentes.\"\n","            return self._handle_faq_path(query, state, logs)\n","\n","        # If the issue is resolved, collect feedback\n","        send_message_to_the_user(\"Que bom que sua questão tenha sido resolvida! Agora, poderia nos fornecer uma avaliação rápida para melhorar o atendimento?\")\n","        self.chat_history += \"\\n\\033[36mSistema\\033[0m: Que bom que sua questão tenha sido resolvida! Agora, poderia nos fornecer uma avaliação rápida para melhorar o atendimento?\"\n","\n","        return self._collect_feedback(query, state, logs)\n","\n","    def _handle_human_escalation_path(self, query, state, logs):\n","\n","        # Module 5 - Sentiment\n","        self.sequence_of_modules.append(5)\n","        module5 = Module5_SentimentAnalysis()\n","        start_time = time.time()\n","        sentiment = module5.sentiment_analysis(query)\n","        logs.append(self._log_execution(5, start_time, sentiment))\n","        state['sentiment'] = sentiment\n","\n","        # Module 4 - Human Escalation\n","        self.sequence_of_modules.append(4)\n","        module4 = Module4_HumanEscalation()\n","        start_time = time.time()\n","        issue_data = module4.run(self.chat_history, sentiment)\n","        logs.append(self._log_execution(4, start_time, issue_data))\n","\n","        # Module 7 - Knowledge Base\n","        self.sequence_of_modules.append(7)\n","        insurance_type = issue_data.get('insurance_type', 0)\n","        module7 = Module7_KnowledgeBaseBuilder(self.chat_history, insurance_type)\n","        start_time = time.time()\n","        kb_update = module7.generate_draft_faq()\n","        logs.append(self._log_execution(7, start_time, kb_update))\n","\n","        # if issue_data[\"human_attendant_name\"] = \"\", the customer was added to the waiting list\n","        if issue_data[\"human_attendant_name\"] == \"\":\n","            send_message_to_the_user(\"Não há nenhum atendente disponível no momento. Você será adicionado à fila de espera.\")\n","            self.chat_history += \"\\n\\033[36mSistema\\033[0m: Não há nenhum atendente disponível no momento. Você será adicionado à fila de espera.\"\n","            return state\n","\n","        human_attendant_name = issue_data[\"human_attendant_name\"]\n","        human_attendant_id = issue_data[\"human_attendant_id\"]\n","        state['human_attendant_name'] = human_attendant_name\n","        state['human_attendant_id'] = human_attendant_id\n","\n","        send_message_to_the_user(f\"Você está sendo encaminhado para o(a) atendente {human_attendant_name}. Aguarde um instante.\")\n","\n","        send_message_to_the_user(issue_data[\"recommended_message\"])\n","        self.chat_history += f\"\\n\\033[34mModulee Humano ({human_attendant_name})\\033[0m: {issue_data['recommended_message']}\"\n","\n","        ### Human Module interacts with the customer ###\n","\n","        # After interaction is done, make the human Module available to help other customers\n","        module4.free_human_agent(human_attendant_id)\n","\n","        sentiment_analysis = issue_data.get('sentiment_analysis', {})\n","        issue_summary = issue_data.get('issue_summary', \"\")\n","        query_category = issue_data.get('query_category', \"\")\n","        query_subcategory = issue_data.get('query_subcategory', \"\")\n","\n","        return self._collect_feedback(query, state, logs, sentiment_analysis, insurance_type, issue_summary, query_category, query_subcategory, human_attendant_name)\n","\n","    def _handle_compliance_failure_faq_path(self, query, state, logs):\n","        \"\"\"\n","        Returns:\n","            rag_response:str: The response from the RAG module. Or an empty string\n","              if the RAG module fails to generate a compliant response.\n","        \"\"\"\n","        # Try once again to generate the answer\n","\n","        # Module 2 - RAG\n","        self.sequence_of_modules.append(2)\n","        module2 = Module2_RAG()\n","        start_time = time.time()\n","        rag_response:str = module2.process_question(query)\n","        logs.append(self._log_execution(2, start_time, rag_response))\n","        state['rag_response_after_compliance_retry'] = rag_response\n","\n","        # Check if the new response is compliant\n","\n","        # Module 9 - Compliance\n","        self.sequence_of_modules.append(9)\n","        module9 = Module9_ComplianceVerifier(query, rag_response)\n","        start_time = time.time()\n","        compliance_check:dict[bool,str] = module9.run_verification()\n","        logs.append(self._log_execution(9, start_time, compliance_check))\n","\n","        if not compliance_check['compliance']:\n","            return \"\"\n","\n","        return rag_response\n","\n","    def _handle_compliance_failure_contact_info_path(self, query, state, logs):\n","        \"\"\"\n","        Returns:\n","            contact_response:str: The response from the contact info module. Or an empty string\n","              if the contact info module fails to generate a compliant response.\n","        \"\"\"\n","        # Try once again to generate the answer\n","\n","        # Module 3 - Contact Info\n","        self.sequence_of_modules.append(3)\n","        module3 = Module3_ContactInfo()\n","        start_time = time.time()\n","        contact_response, chat_history_module3 = module3.ask_module(query)\n","        logs.append(self._log_execution(3, start_time, contact_response))\n","        state['contact_response_after_compliance_retry'] = contact_response\n","\n","        # Check if the new response is compliant\n","\n","        # Module 9 - Compliance\n","        self.sequence_of_modules.append(9)\n","        module9 = Module9_ComplianceVerifier(query, contact_response)\n","        start_time = time.time()\n","        compliance_check:dict[bool,str] = module9.run_verification()\n","        logs.append(self._log_execution(9, start_time, compliance_check))\n","\n","        if not compliance_check['compliance']:\n","            return \"\"\n","\n","        return contact_response\n","\n","\n","    def _collect_feedback(self, query, state, logs, sentiment_analysis:dict={}, insurance_type:int=0, issue_summary:str=\"\", query_category:str=\"\", query_subcategory:str=\"\", human_attendant_name:str=\"\"):\n","        # Module 6 - Feedback\n","        self.sequence_of_modules.append(6)\n","        module6 = Module6_FeedbackCollector(self.chat_history, human_attendant_name, sentiment_analysis, insurance_type, issue_summary, query_category, query_subcategory)\n","        start_time = time.time()\n","        feedback = module6.get_customer_feedback()\n","        logs.append(self._log_execution(6, start_time, feedback[\"general\"]))\n","        state['general_feedback'] = feedback[\"general\"]\n","        return state"]},{"cell_type":"markdown","metadata":{"id":"cXJ2luztH_za"},"source":["### 3.3.2 Workflow Examples"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57370,"status":"ok","timestamp":1746278401596,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"Dgkag4WXtHK3","outputId":"eafcb376-ddb2-47d7-cd3f-21f385032321"},"outputs":[{"name":"stdout","output_type":"stream","text":["Obrigado pela pergunta! Estamos buscando uma resposta para ela no banco de perguntas frequentes.\n","Resposta: Olá, José! De acordo com as informações fornecidas, o plano Bradesco Dental é um convênio odontológico que garante acesso a tratamentos odontológicos básicos, como limpeza e tratamento de cáries, com uma ampla cobertura nacional de mais de 27 mil dentistas credenciados. Ele funciona de forma semelhante a um plano de saúde, oferecendo uma variedade de procedimentos, incluindo extração, canal e cirurgias realizadas em consultório. Você pode encontrar mais informações sobre a cobertura completa do plano no portal do Bradesco.\n","Fontes (ids das FAQs): [41, 36, 42, 38, 40]\n","Espero ter ajudado, José! Você conseguiu entender como funciona o seguro dental Bradesco? (S/N): \n","S\n","Que bom que sua questão tenha sido resolvida! Agora, poderia nos fornecer uma avaliação rápida para melhorar o atendimento?\n","Quão satisfeito você está com a resolução do seu problema? (1-5): 5\n","Como você avaliaria o atendimento do agente? (1-5): 5\n","Quão fácil foi interagir com o sistema? (1-5): 5\n","Como você avaliaria sua experiência geral? (1-5): 4\n","Você gostaria de fazer algum comentário adicional sobre o sua experiência? Adorei a forma rápida e eficiente com que minha dúvida foi solucionada.\n"]}],"source":["query = \"Eu sou o José e gostaria de saber como funciona o seguro dental Bradesco.\" # 0\n","# query = \"Onde posso ligar para renovar meu seguro de Saúde?\" # 1\n","# query = \"Eu sou o Gabriel, do plano de seguro de vida. Preciso da ajuda de um atendente urgentemente.\" # 2\n","# query = \"Qual é o maior animal do mundo?\" # 3 - Pergunta não pertinente\n","\n","workflow = WorkflowManager()\n","final_state_and_logs = workflow.process_query(query)"]},{"cell_type":"markdown","metadata":{"id":"5V1aIVEEITlq"},"source":["### 3.3.3 Log example (chat history, sequence of modules, latency of each module, etc.)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":3,"status":"ok","timestamp":1746278404551,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"YaMCOXs2C9V7","outputId":"a1fecc9b-8b02-41b0-a391-f3612888fd3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","    \"chat_history\": \"\\n\\u001b[32mUser\\u001b[0m: Eu sou o José e gostaria de saber como funciona o seguro dental Bradesco.\\n\\u001b[36mSistema\\u001b[0m: Estamos buscando uma resposta para ela no banco de perguntas frequentes.\\n\\u001b[34mModulo 2\\u001b[0m: Resposta: Olá, José! De acordo com as informações fornecidas, o plano Bradesco Dental é um convênio odontológico que garante acesso a tratamentos odontológicos básicos, como limpeza e tratamento de cáries, com uma ampla cobertura nacional de mais de 27 mil dentistas credenciados. Ele funciona de forma semelhante a um plano de saúde, oferecendo uma variedade de procedimentos, incluindo extração, canal e cirurgias realizadas em consultório. Você pode encontrar mais informações sobre a cobertura completa do plano no portal do Bradesco.\\nFontes (ids das FAQs): [41, 36, 42, 38, 40]\\n\\u001b[34mModule 8\\u001b[0m: Espero ter ajudado, José! Você conseguiu entender como funciona o seguro dental Bradesco?\\n\\u001b[32mUser\\u001b[0m: S\\n\\u001b[36mSistema\\u001b[0m: Que bom que sua questão tenha sido resolvida! Agora, poderia nos fornecer uma avaliação rápida para melhorar o atendimento?\",\n","    \"sequence_of_modules\": [\n","        1,\n","        2,\n","        9,\n","        8,\n","        6\n","    ],\n","    \"final_state\": {\n","        \"classification\": 0,\n","        \"rag_response\": \"Resposta: Olá, José! De acordo com as informações fornecidas, o plano Bradesco Dental é um convênio odontológico que garante acesso a tratamentos odontológicos básicos, como limpeza e tratamento de cáries, com uma ampla cobertura nacional de mais de 27 mil dentistas credenciados. Ele funciona de forma semelhante a um plano de saúde, oferecendo uma variedade de procedimentos, incluindo extração, canal e cirurgias realizadas em consultório. Você pode encontrar mais informações sobre a cobertura completa do plano no portal do Bradesco.\\nFontes (ids das FAQs): [41, 36, 42, 38, 40]\",\n","        \"resolution_check_faq_path\": true,\n","        \"general_feedback\": {\n","            \"rating\": 5,\n","            \"follow_up_response\": \"Adorei a forma rápida e eficiente com que minha dúvida foi solucionada.\",\n","            \"categories\": [\n","                \"Experiência Positiva\"\n","            ]\n","        }\n","    },\n","    \"execution_logs\": [\n","        {\n","            \"module\": 1,\n","            \"execution_time\": 0.8159618377685547,\n","            \"timestamp\": \"2025-05-03T13:19:03.086675\",\n","            \"response\": 0\n","        },\n","        {\n","            \"module\": 2,\n","            \"execution_time\": 4.880731582641602,\n","            \"timestamp\": \"2025-05-03T13:19:07.967554\",\n","            \"response\": \"Resposta: Olá, José! De acordo com as informações fornecidas, o plano Bradesco Dental é um convênio odontológico que garante acesso a tratamentos odontológicos básicos, como limpeza e tratamento de cáries, com uma ampla cobertura nacional de mais de 27 mil dentistas credenciados. Ele funciona de forma semelhante a um plano de saúde, oferecendo uma variedade de procedimentos, incluindo extração, canal e cirurgias realizadas em consultório. Você pode encontrar mais informações sobre a cobertura completa do plano no portal do Bradesco.\\nFontes (ids das FAQs): [41, 36, 42, 38, 40]\"\n","        },\n","        {\n","            \"module\": 9,\n","            \"execution_time\": 0.2030045986175537,\n","            \"timestamp\": \"2025-05-03T13:19:08.174409\",\n","            \"response\": {\n","                \"compliance\": true,\n","                \"violation\": \"Sem violação\"\n","            }\n","        },\n","        {\n","            \"module\": 8,\n","            \"execution_time\": 0.44731760025024414,\n","            \"timestamp\": \"2025-05-03T13:19:08.626075\",\n","            \"response\": \"Espero ter ajudado, José! Você conseguiu entender como funciona o seguro dental Bradesco?\"\n","        },\n","        {\n","            \"module\": 6,\n","            \"execution_time\": 46.53120565414429,\n","            \"timestamp\": \"2025-05-03T13:19:59.279551\",\n","            \"response\": {\n","                \"rating\": 5,\n","                \"follow_up_response\": \"Adorei a forma rápida e eficiente com que minha dúvida foi solucionada.\",\n","                \"categories\": [\n","                    \"Experiência Positiva\"\n","                ]\n","            }\n","        }\n","    ]\n","}\n"]}],"source":["print(json.dumps(final_state_and_logs, indent=4, ensure_ascii=False))"]},{"cell_type":"markdown","metadata":{"id":"F1N7WpKt_wXh"},"source":["## 3.2 HUMAN AGENT (Service list)\n","\n","Choose the correct human agent based on the query category and insurance type"]},{"cell_type":"markdown","metadata":{"id":"qMEQcf_fM7o3"},"source":["### 3.2.1 Human_agent class - for the human agent to interact with the customer"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"oZuXqKM5BoBx","executionInfo":{"status":"ok","timestamp":1746278418919,"user_tz":180,"elapsed":2,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["insurance_types = {\n","    0: \"Standart\",\n","    1: \"Auto\",\n","    2: \"Health\",\n","    3: \"Life\",\n","    4: \"Dental\",\n","    5: \"Social Security\",\n","    6: \"Residential\",\n","}\n","category_mapping = [\"Policy Management\", \"Claims\", \"Payments\", \"General Questions\", \"Technical Problems\", \"Human Support Escalations\", \"Regulatory or Compliance Questions\"]\n","\n","class Human_agent:\n","    def __init__(self, human_attendant_name, insurance_type:int=0, query_category:str=\"General Questions\",\n","                 status:str=\"Available\",\n","                 waiting_list_path:str=f\"{STELLAR_path}/outputs/module_4/waiting_list.json\"):\n","        self.human_attendant_name = human_attendant_name\n","        self.insurance_type = insurance_type\n","        self.status = status\n","        self.query_category = query_category\n","        self.waiting_list_path = waiting_list_path\n","\n","    def find_customer(self):\n","        \"\"\"\n","        This is the function a human agent can use to find a customer in the waiting list.\n","        \"\"\"\n","        try:\n","            with open(self.waiting_list_path, \"r\") as f:\n","                waiting_list = json.load(f)\n","        except FileNotFoundError:\n","            waiting_list = []\n","\n","        if len(waiting_list) == 0:\n","            print(\"There are no appointments in the queue.\")\n","            return None\n","\n","        # if there is a customer that is a perfect match for this agent, serve this customer\n","        for i in range(len(waiting_list)):\n","            if waiting_list[i][\"insurance_type\"] == self.insurance_type and waiting_list[i][\"query_category\"] == self.query_category:\n","                customer = waiting_list.pop(i)\n","                with open(self.waiting_list_path, \"w\") as f:\n","                    json.dump(waiting_list, f, indent=2, ensure_ascii=False)\n","                self.customer_service(customer)\n","                return\n","\n","        # Check for partial matches\n","        for i in range(len(waiting_list)):\n","          if waiting_list[i][\"insurance_type\"] == self.insurance_type:\n","              customer = waiting_list.pop(i)\n","              with open(self.waiting_list_path, \"w\") as f:\n","                  json.dump(waiting_list, f, indent=2, ensure_ascii=False)\n","              self.customer_service(customer)\n","              return\n","\n","          if waiting_list[i][\"query_category\"] == self.query_category:\n","              customer = waiting_list.pop(i)\n","              with open(self.waiting_list_path, \"w\") as f:\n","                  json.dump(waiting_list, f, indent=2, ensure_ascii=False)\n","              self.customer_service(customer)\n","              return\n","\n","        # if there is no partial match, serve the customer with the highest urgency in the waiting list\n","        waiting_list.sort(key=lambda x: x[\"urgency_score\"], reverse=True)\n","        customer = waiting_list.pop(0)\n","        with open(self.waiting_list_path, \"w\") as f:\n","            json.dump(waiting_list, f, indent=2, ensure_ascii=False)\n","        self.customer_service(customer)\n","        return\n","\n","    def customer_service(self, issue_data:dict):\n","        \"\"\"\n","        This is how the human agent interacts with the customer. First, he receives the issue data,\n","        and then he can start the interaction with the customer.\n","        Args:\n","            issue_data (dict): a dict with the issue data in this format:\n","            {\"sentiment\": <dict>, \"chat_history\": <str>, \"human_attendant_name\": <str>, \"model\": <str>,\n","            \"customer_name\": <str>, \"insurance_type\": <int>, \"issue_summary\": <str>, \"query_category\": <str>,\n","            \"query_subcategory\": <str>, \"urgency_score\": <int>, \"recommended_message\": <str>}\n","        \"\"\"\n","        print(f\"\\033[34mHuman Module ({issue_data['human_attendant_name']})\\033[0m: {issue_data['recommended_message']}\")\n","\n","        # From now on, the human agent should take control of the interaction\n","        return"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1746278420921,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"},"user_tz":180},"id":"Z7dneuF6X2vw","outputId":"63c90662-42e2-46a7-8b0f-1c76583f3ac7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34mHuman Module (Letícia)\u001b[0m: Hello Doug, this is Letícia, your attendant from the health insurance area of Bradesco Seguros. I'm here to listen and help you with any questions or concerns you may have regarding your health insurance. I'd love to understand what's on your mind and how I can assist you today. What would you like to talk about regarding your health insurance coverage?\n"]}],"source":["human_agent = Human_agent(\"Marcos\", insurance_type=0, query_category=\"Perguntas Gerais\")\n","human_agent.find_customer()"]},{"cell_type":"markdown","metadata":{"id":"32tvO132M6oB"},"source":["### 3.2.2 Creating some examples of human agents"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"Qa_iYO-JNNqf","executionInfo":{"status":"ok","timestamp":1746278428061,"user_tz":180,"elapsed":43,"user":{"displayName":"Matheus Scatolin","userId":"11486090880360902910"}}},"outputs":[],"source":["human_agents = []\n","human_agents.append(Human_agent(\"Marcos\", insurance_type=0, query_category=\"General Questions\"))\n","human_agents.append(Human_agent(\"Maria\", insurance_type=1, query_category=\"Claims\"))\n","human_agents.append(Human_agent(\"João\", insurance_type=2, query_category=\"Policy Management\"))\n","human_agents.append(Human_agent(\"Ana\", insurance_type=3, query_category=\"Payments\"))\n","human_agents.append(Human_agent(\"Pedro\", insurance_type=4, query_category=\"General Questions\"))\n","human_agents.append(Human_agent(\"Carla\", insurance_type=5, query_category=\"Human Support Escalations\"))\n","human_agents.append(Human_agent(\"Lucas\", insurance_type=6, query_category=\"Regulatory or Compliance Questions\"))\n","human_agents.append(Human_agent(\"Fernanda\", insurance_type=0, query_category=\"Claims\"))\n","human_agents.append(Human_agent(\"Rafael\", insurance_type=1, query_category=\"Policy Management\"))\n","human_agents.append(Human_agent(\"Isabela\", insurance_type=2, query_category=\"Payments\"))\n","human_agents.append(Human_agent(\"Gustavo\", insurance_type=3, query_category=\"General Questions\"))\n","human_agents.append(Human_agent(\"Juliana\", insurance_type=4, query_category=\"Human Support Escalations\"))\n","human_agents.append(Human_agent(\"Diego\", insurance_type=5, query_category=\"Regulatory or Compliance Questions\"))\n","human_agents.append(Human_agent(\"Larissa\", insurance_type=6, query_category=\"Claims\"))\n","human_agents.append(Human_agent(\"Bruno\", insurance_type=0, query_category=\"Policy Management\"))\n","human_agents.append(Human_agent(\"Amanda\", insurance_type=1, query_category=\"Payments\"))\n","human_agents.append(Human_agent(\"Ricardo\", insurance_type=2, query_category=\"General Questions\"))\n","human_agents.append(Human_agent(\"Camila\", insurance_type=3, query_category=\"Human Support Escalations\"))\n","human_agents.append(Human_agent(\"Gabriel\", insurance_type=4, query_category=\"Regulatory or Compliance Questions\"))\n","human_agents.append(Human_agent(\"Renata\", insurance_type=5, query_category=\"Claims\"))\n","human_agents.append(Human_agent(\"Felipe\", insurance_type=6, query_category=\"Policy Management\"))\n","human_agents.append(Human_agent(\"Lívia\", insurance_type=0, query_category=\"Payments\"))\n","human_agents.append(Human_agent(\"Matheus\", insurance_type=1, query_category=\"General Questions\"))\n","human_agents.append(Human_agent(\"Letícia\", insurance_type=2, query_category=\"Human Support Escalations\"))\n","human_agents.append(Human_agent(\"Vinicius\", insurance_type=3, query_category=\"Regulatory or Compliance Questions\"))\n","\n","module4 = Module4_HumanEscalation()\n","# To add the new agents, remove the comments bellow:\n","#for human_agent in human_agents:\n","#    module4.add_human_agent(human_agent)\n"]}],"metadata":{"colab":{"collapsed_sections":["QAy-DGlJXirS","RDaSsXz0YPfO","JOIX25GqZTCB","ytSguSoejHRN","FMEf2c9iogFN","5AHa-S-hG7tl","X8z8ygPWMQvx","Suddcf6bRY9z","DOsKCQFazH5q","aJCjG7U70iAZ","_7ogq68YH4R-","cXJ2luztH_za","5V1aIVEEITlq","qMEQcf_fM7o3","32tvO132M6oB"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}